---
title: "BKA_Kriminalstatistik_2019_data_analysis"
author: "Peter von Bodelschwingh, Joshua Gawenda"
date: "12 12 2020"
output:
  pdf_document: default
  pdf: default
---

```{r setup, include=FALSE}
library(readxl)
library(dplyr)
library(sjmisc)
library(tidyverse)
library(GDINA)
library(geojsonio)
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction
This dataset is taken from the  Bundeskriminalamt - the leading institution of crime detection in germany.
The original dataset can be found here: 
[BKA Dataset](https://www.bka.de/DE/AktuelleInformationen/StatistikenLagebilder/PolizeilicheKriminalstatistik/PKS2019/PKSTabellen/LandFalltabellen/landFalltabellen.html?nn=130872)

It contains data about all crimes recorded in 2019. This analysis divides the statistics into all 16 german provinces. The goal of this analysis is to present the differences between these provinces by categories of crime, population, sucessful and non successfull enlisted crimes. Further documentation is provided in the "Interpretationshilfen" subfolder of this project.  

The original excel file can be found in this repository as well. See "PKS_crime_statistics.xlsx". 


## Import
The first step we did was to divide the Overview Sheet into subsheets that contain all data by the province. In order to make sure the importing into R is done correctly, we removed line one til seven so only the Nr of the column is recognized as header, by limiting the range. 
 
```{r}
library(readxl)
originalSet <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Data",
range = "A8:U18777") 
sumKeysImport <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Sumkeys")

```

## Rename columns
as you can see here the column names are not that descriptive anymore: 
```{r}
names(originalSet)
```
Thats why we are going to rename the columns back to the original name of the dataset now. 
```{r}
originalSet <- originalSet %>% 
                rename(
                  key = '1',
                  crime = '2',
                  province = '3',
                  detected_cases = '4',
                  detected_cases_percent = '5',
                  not_executed_cases = '6',
                  not_executed_cases_percent = '7',
                  distribution_under_20K = '8',
                  distribution_20K_to_100K = '9',
                  distribution_100K_to_500K = '10',
                  distribution_over_500K = '11',
                  distribution_unknown = '12',
                  gun_threatened = '13',
                  gun_shot = '14',
                  solved_cases = '15',
                  solved_cases_percent = '16',
                  suspects = '17',
                  male = '18',
                  female = '19',
                  non_german ='20',
                  non_german_percent = '21'
                  )
```
 

### Decouple

First step: remove "N" (TODO determin) values, some keys are safed as "NA", they will also be removed,because we keep "Y" values only.
```{r}
singleKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key))
sumKeys <- sumKeysImport %>% filter(!grepl("Y",is_sum_key))
```



Remove keys that determine sum values:
TODO: add more documentation, explain sumkeys, explain retrieval process 

```{r}
filteredKeys <- c()
visited <- c()
for (key1 in singleKeys$key) {
  visited <- append(visited, key1)
  for (key2 in singleKeys$key) {
    if (!key2 %in% visited) {
      if (regexpr(paste("^", str_remove(key2, "0+$"), sep = ""), key1)[1] == 1) {
        filteredKeys <- append(filteredKeys, key2)
      }
    }
  }
}
```

Manual check for real sum keys
```{r}
singleKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key) & !(key %in% c("141100","231200","133000")))
sumKeys <- sumKeysImport %>% filter(!grepl("Y",is_sum_key) | (key %in% c("141100","231200","133000")))
```


```{r}
singleValues <- merge(originalSet, singleKeys, by = "key")
sumValues <- merge(originalSet, sumKeys, by = "key")
``` 

```{r}
germanSingleSet <-
  singleValues %>% filter(grepl("Bundesrepublik Deutschland", province))
provinceSingleSet <-
  singleValues %>% filter(!grepl("Bundesrepublik Deutschland", province))

germanSumSet <-
  sumValues %>% filter(grepl("Bundesrepublik Deutschland", province))
provinceSumSet <-
  sumValues %>% filter(!grepl("Bundesrepublik Deutschland", province))

sum(germanSingleSet$detected_cases)
sum(provinceSingleSet$detected_cases)
sum(germanSumSet$detected_cases)
sum(provinceSumSet$detected_cases)
```

## Tidy
### Check non null
First of all we proof that the original Set does not contain any null values: 
```{r}
nonNullValueOriginal <-?
  originalSet %>% filter_at(vars(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21),
                            any_vars(is.na(.)))
nrow(nonNullValueOriginal) == 0

nonNullValueOriginal <-
  germanSingleSet %>% filter_at(vars(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21),
                            any_vars(is.na(.)))
nrow(nonNullValueOriginal) == 0

nonNullValueOriginal <-
  provinceSingleSet %>% filter_at(vars(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21),
                            any_vars(is.na(.)))
nrow(nonNullValueOriginal) == 0

nonNullValueOriginal <-
  germanSumSet %>% filter_at(vars(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21),
                            any_vars(is.na(.)))
nrow(nonNullValueOriginal) == 0

nonNullValueOriginal <-
  provinceSumSet %>% filter_at(vars(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21),
                            any_vars(is.na(.)))
nrow(nonNullValueOriginal) == 0
```

According to the three interrelated rules this set can be considered as tidy now because:
    Each variable has its own column.
    Each observation has its own row.
    Each value has its own cell.

### Remove calculated columns
Since we want to keep our analysis as performant as possible we also remove all calculated columns:

```{r}
germanSingleSet <- 
  germanSingleSet %>% select(1:4, 15, 6, 8:14, 17:20)
provinceSingleSet <-
  provinceSingleSet %>% select(1:4, 15, 6, 8:14, 17:20)

germanSumSet <- germanSumSet %>% select(1:4, 15, 6, 8:14, 17:20)
provinceSumSet <- provinceSumSet %>% select(1:4, 15, 6, 8:14, 17:20)
```

Add a number to a provinve to merge with geojson files later on 
```{r}
df <- data.frame (province  = c("Baden-Württemberg","Bayern","Berlin","Brandenburg","Bremen","Hamburg","Hessen","Mecklenburg-Vorpommern","Niedersachsen","Nordrhein-Westfalen","Rheinland-Pfalz","Saarland","Sachsen-Anhalt","Sachsen","Schleswig-Holstein","Thüringen"),
                  id = c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16"))
df
```
Shorten Set
```{r}
shortenedSingleSet <- select(provinceSingleSet,province, detected_cases)

#sum
shortenedSingleSet <- aggregate(x = shortenedSingleSet$detected_cases,                # Specify data column
          by = list(shortenedSingleSet$province),              # Specify group indicator
          FUN = sum)

#rename
shortenedSingleSet <- shortenedSingleSet %>% rename(
          province = "Group.1",
          sumOfCases = x)
```

Merge singleset to ids 
```{r}
shortenedSingleSet <- merge(shortenedSingleSet,df, by = "province")
```





## Transform
## Visualise 
## Model 


## german provinces
```{r}
library(geojsonio)
spdf <- geojson_read("provinces.json", what = "sp")
length(spdf)
```


```{r}
spdf
```


#plot(spdf)
# I need to fortify the data AND keep trace of the commune code! (Takes ~2 minutes)
```{r}
library(broom)
spdf_fortified <- tidy(spdf, area = "code")

```

# Now I can plot this shape easily as described before:
```{r}

library(ggplot2)

ggplot() +
  geom_polygon(data = spdf_fortified, aes( x = long, y = lat, group = group), fill="white", color="grey") +
  theme_void() +
  coord_map()
```

```{r}
data <- shortenedSingleSet 
data %>%
  ggplot( aes(x=sumOfCases)) +
    geom_histogram(bins=60, fill='skyblue', color='#69b3a2') + scale_x_log10()
```

Merge GeoJson with the amount of cases 
```{r}
spdf_fortified = spdf_fortified %>%
  left_join(. , data, by="id")
```

```{r}
ggplot() +
  geom_polygon(data = spdf_fortified, aes(fill = sumOfCases, x = long, y = lat, group = group)) +
  theme_void() +
  coord_map()
```

As can be seen in the german map, the amount of cases is wrong. This happened in the join process. For each row of the JSON set, a sum of the province was added. So we have to count the nr of rows per province now to divide the sum of cases by this value in order to get the correct value. 
```{r}
#count
testCount <- aggregate(x = spdf_fortified$sumOfCases,                # Specify data column
          by = list(spdf_fortified$province),              # Specify group indicator
          FUN = length)
```

```{r}
options(scipen=999)
summary(germanSingleSet$detected_cases)
summary(germanSingleSet$solved_cases)
summary(germanSingleSet$non_german)
summary(germanSingleSet$suspects)

```

# zunächst die graphische Darstellung mittels Punktwolke 
```{r}
plot(germanSingleSet$detected_cases, germanSingleSet$solved_cases, xlab = "Detected cases", ylab = "Solved cases",
     #main = "", xlim = c(35,47), ylim = c(150,195), pch = 20) 
  , pch = 20) 
     
```

# die einfache lineare Regression lässt sich über den sehr 
# wichtigen (und mächtigen) Befehl 'lm()' (linear model)
# anpassen; mit diesem Befehl lassen sich eine Unmenge an statistischen
# Modellen durchführen, indem eine 'Formel' spezifiziert wird
# allgemein hat der Befehl folgende Struktur: lm(<Kriteriumsvariable> ~ <Prädiktor(en)>)

# wir wollen nun die einfache lineare Regression durchführen und 
# das Ergebnis in dem Objekt 'reg' speichern
```{r}
reg<-lm(germanSingleSet$detected_cases ~ germanSingleSet$solved_cases)

```

# was bekommt man?
```{r}
reg 
```

# dieses Objekt enthält offenbar den Schnittpunkt der zugehörigen Regressionsgerade
# mit der y-Achse (Intercept) und den Anstieg der Regressionsgerade (mit dem Namen
# der Prädiktorvariable bezeichnet)

# zur Kontrolle: wenn man Anstieg (a) und Achsenabschnitt (b) 'per Hand'
# berechnet (siehe letztes Semester), kommt man zu genau diesen Werten
```{r}
a<-cov(germanSingleSet$detected_cases, germanSingleSet$solved_cases)/var(germanSingleSet$solved_cases)
b<-mean(germanSingleSet$detected_cases) - a*mean(germanSingleSet$solved_cases)

```


# das Objekt 'reg' lässt sich auch einfach in den bereits aus den Übungsaufgaben
# bekannten Befehl 'abline()' einsetzen, um die Regressionsgerade in die 
# bereits erstellte Punktwolke einzuzeichnen
```{r}
abline(reg)

```

# das Objekt 'reg' (also das Ergebnis der angepassten einfachen lin. Regression)
# enthält jedoch noch vieles mehr
# was alles enthalten ist (quasi ein Überblick über das angepasste Modell)
# lässt sich mit Hilfe des Bereits bekannten Befehls 'summary()' anzeigen
```{r}
summary(reg)

```

# das meiste davon brauchen wir an dieser Stelle noch nicht (es werden u.a.
# die Ergebnisse von Signifikanztests über Anstieg und Achsenabschnitt
# der Regressionsgerade durchgeführt, mehr dazu später), was Sie aber vielleicht
# wiedererkennen, ist (neben einer Übersicht zu den Residuen zu Beginn) 
# der Determinationskoeffizient, der hier unter 'Multiple R-squared'
# aufgeführt ist 
# -> ca. 77% der Varianz der Körpergröße wird durch das Regressionsmodell 'erklärt'

# die Struktur dieses Objektes 'reg' ist eine sogenannte Liste, die komplexeste Art
# von Objekten in R; 
str(reg)

# anschaulich kann man sich eine Liste als ein Objekt vorstellen,
# das als Elemente selbst wieder Objekte haben kann

# wir können uns hier zum Beispiel sämtliche 'vorhergesagten Werte' y_dach
# anzeigen lassen 
```{r}
reg$fitted.values

```

# oder die zugehörigen Residuen = Fehler (Abweichung der Geraden)
```{r}
reg$residuals

```

# wir können also auch einen Residuenplot erstellen
```{r}
plot(daten$Schuhgröße,reg$residuals, pch = 20, ylab = "Residuen", xlab = "Schuhgröße")
abline(h = 0)

```



