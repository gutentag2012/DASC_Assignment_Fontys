---
title: "BKA_Kriminalstatistik_2019_data_analysis"
author: "Peter von Bodelschwingh, Joshua Gawenda"
date: "12 12 2020"
output:
  html_document:
    df_print: paged
  pdf: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(dplyr)
library(sjmisc)
library(tidyverse)
library(readxl)
library(GDINA)
```

# German crime statistics 2019 - A data analysis

## Introduction

In this data analysis we are going to take a look at the German crime statistics from the year 2019, the special version we are looking at also
shows the crime distribution about different sized cities. The data is divided for all 16 German provinces and into a general section for the whole
country. \n
There are also multiple files that help to determine what can be deducted from a data analysis of this set and what is out of scope. \n
The data set is taken from the  "Bundeskriminalamt" - the leading institution of crime detection in Germany. \n
The original data set can be found here: 
[BKA Dataset](https://www.bka.de/DE/AktuelleInformationen/StatistikenLagebilder/PolizeilicheKriminalstatistik/PKS2019/PKSTabellen/LandFalltabellen/landFalltabellen.html) \n
This data set only shows absolute numbers for all provinces in Germany, so to make the statistics comparable over all the different provinces we
also included another data set, that shows the number of inhabitants for every province and the whole country. \n
This data set can be found here: 
[German Inhabitants](https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.html) \n
Later you will see that we imported on more data set. This one is generated out of a PDF from the documentation for the first data set and therefore
is not available in the format we have online. It holds information about the so called sum keys of the main data set and is later used to decouple
the main data set.

### Sum Keys

To understand a big part of the analysis you have to understand what sum keys are, this is what we are going to describe in this chapter. \n
A key in the data set we use represents one type of crime, every crime has its own distinct key. Some crimes are just a composite of multiple other
crimes, their key is then called a sum key, so every crime that is not a sum, has no children but only a parent crime. \n 
A key is made out of six digits, where every digit is read as a single one from left to right. In general every key that starts with the same digits 
is part of the same sum key, the sum key is then identified with the same starting digits followed by zeros. However this is not true in every case, 
some keys that end with zero have no children and therefore are no sum keys, the sum key for those keys is in most cases represented by a "*"-sign. \n
One general section of crimes is therefore represented by only the first digit. There are also two whole sections of keys that are only made out of
sum keys and are used by the police to study different fields of crimes.

## Import

The first step to every data analysis is to import the data into your project. The data we chose is available as an excel file so we use the 
package "readxl" to import the sets from excel. For the import to work properly we had to remove some unnecessary header lines in the main data
set, we did that by specifying a "range" in the read_excel command. \n
The "SumKeys" data set was created by us, therefore every optimization for importing where made while creating the set. \n 
Finally we also import the data set for the inhabitants, since this data set is not part of the main analysis, we decided to also edit it in
excel to optimize it for the import and remove data that we do not need for the analysis.
 
```{R Import}
originalSet <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Data",range = "A8:U18777") 
sumKeysImport <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Sumkeys")
germanInhabitants <- read_excel("DataSets/02-bundeslaender.xlsx", sheet = "inhabitants")
```

### Renaming

After the import, the names of the original data set got lost and are not descriptive anymore. To assure that the usage later on is more understandable
we renamed the data set.

```{r Renaming}
originalSet <- originalSet %>% 
                rename(
                  key = '1',
                  crime = '2',
                  province = '3',
                  detected_cases = '4',
                  detected_cases_percent = '5',
                  not_executed_cases = '6',
                  not_executed_cases_percent = '7',
                  distribution_under_20K = '8',
                  distribution_20K_to_100K = '9',
                  distribution_100K_to_500K = '10',
                  distribution_over_500K = '11',
                  distribution_unknown = '12',
                  gun_threatened = '13',
                  gun_shot = '14',
                  solved_cases = '15',
                  solved_cases_percent = '16',
                  suspects = '17',
                  male = '18',
                  female = '19',
                  non_german ='20',
                  non_german_percent = '21'
                  )
```
 
As a last step of importing, we can merge the inhabitants data frame into the original data frame for the analysis. This would allow to use the
number of inhabitants easily in any calculation from here on.

```{r Merge_Inhabitants}
originalSet <- merge(originalSet, germanInhabitants, by = "province") %>% arrange(key)
head(originalSet)
```

## Tidy

This next step includes checking for missing values and in our case splitting the data frames up, so that the provinces are decoupled from the values
for whole Germany. We also need to remove all sum keys from the actual capture keys.

### Check non NA

If the data set contains empty values they are marked as NA and have to be handled somehow. However, this data set has no such values as shown in the
next code chunk. Here we filter for NA values in any column and check if the row count is 0 in the end.

```{r NA_Check}
nrow(filter_at(originalSet, 1:21, any_vars(is.na(.)))) == 0
```

### Decouple

Like describe in the "Sum Key" section, the original data set is a mix of sum keys and capture keys. In order to not count twice in some operations
we remove the sum keys from the capture keys, so the main analysis can focus on the capture keys. \n
The sum keys set that we imported has two columns, one for the key and one with the value "Y" for every capture key and a "N" for every sum key. The 
two key sections that are only made out of sum keys are not reported in the set, so can be handled as "NA".

```{r Filtering_Keys}
captureKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key))
sumKeys <- sumKeysImport %>% filter(!grepl("Y",is_sum_key))
```

To check, that the keys after the filter are all correct, we created a small algorithm that checks for sum keys based on the the rules described in
the "Sum Keys" chapter. The code takes a while to run, that is why we saved the result in a vector. 

```{r Manual Sum_Key_Check}
#filteredKeys <- c()
#for (key1 in captureKeys$key) {
#  if (!key1 %in% filteredKeys) {
#    for (key2 in captureKeys$key) {
#      if (!key2 %in% filteredKeys) {
#        if (key1 != key2) {
#          if (regexpr(paste("^", str_remove(key2, "0+$"), sep = ""), key1)[1] == 1) {
#            filteredKeys <- append(filteredKeys, key2)
#          }
#        }
#      }
#    }
#  }
#}
filteredKeys <- c("133000","141100","231200","305000","310000","315000","325000","326000","335000","340000","345000","350000","390000","435000","436000","620010","655010","670010","670020","670030")
```

After checking the resulting keys manually we found, that the keys "141100", "231200" and "133000" were marked as capture keys, but are sum keys,
so these keys have to be removed from the capture keys and added to the sum keys.

```{r Correct_Key_Filter}
captureKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key) & !(key %in% c("141100","231200","133000")))
sumKeys <- sumKeysImport %>% filter(!grepl("Y",is_sum_key) | (key %in% c("141100","231200","133000")))
```

### Filter final data frames

First to reduce duplicate data, we filtered out all crime descriptions into its own data frame, to it can be looked up afterwards.

```{r Crime_Frame}
crimeOverview <- originalSet %>% select(key, crime) %>% unique()
crimeOverview <- arrange(crimeOverview, key)
head(crimeOverview)

originalSet <- originalSet %>% select(!crime)
```
    
## Transform

In the final step before visualizing the data we need to check all the types of the values in each column an change them to a type that is usable
for our analysis. We can also remove and create new Columns if necessary. In the end we are also splitting the original data frame up based on the
provinces and the sum keys determined before.

### Transforming data types

In this step we first check for the different data types we have in every column. We expect to have characters as the key, a factor for the provinces
and every other column should be a number.

```{r Type_Check}
sapply(originalSet, class)
```
Except for the province all values have the type expected, the next step is to transform the province into a factor.

```{r Factorize_Province}
originalSet$province <- as.factor(originalSet$province) 
sapply(originalSet, class)
```

### Remove calculated columns

Some columns in the data set are just calculated from the given data, so to remove the possibility of a calculation error, we remove these columns
and add them on our own, once they are needed in the analysis.

```{r Select_Useful_Columns}
originalSet <- originalSet %>% select(key, province, inhabitants, detected_cases, solved_cases, not_executed_cases, suspects, male, female, non_german, gun_threatened, gun_shot, distribution_under_20K, distribution_20K_to_100K, distribution_100K_to_500K, distribution_over_500K, distribution_unknown)
```
### Split up data

This could also be part of the tidy step, but to reduce the amount of duplicate code for all checks, we split the data at the end. Therefore, we
merge both key frames together with the original data set to separate the sum keys and the capture keys into separate data frames.

```{r Split_Original_In_Sum_Capture}
captureValues <- merge(originalSet, captureKeys) %>% select(!is_sum_key)
sumValues <- merge(originalSet, sumKeys) %>% select(!is_sum_key)
```

With the sum keys now separated, we also divide both data frames into the values for the provinces and the values for whole Germany.

```{r Separate_Province}
germanCaptureSet <-
  captureValues %>% filter(grepl("Bundesrepublik Deutschland", province))
provinceCaptureSet <-
  captureValues %>% filter(!grepl("Bundesrepublik Deutschland", province))

germanSumSet <-
  sumValues %>% filter(grepl("Bundesrepublik Deutschland", province))
provinceSumSet <-
  sumValues %>% filter(!grepl("Bundesrepublik Deutschland", province))
```

To check that the sum keys from before where separated correct we check that the sum of all detected cases in each German set is equal to the sum 
for the detected cases in the province set.

```{r Sum_Key_Check}
sum(germanCaptureSet$detected_cases) == sum(provinceCaptureSet$detected_cases)
sum(germanSumSet$detected_cases) == sum(provinceSumSet$detected_cases)
```

```{r, echo=FALSE}
options(scipen=999)
```
    
## Visualise 
############################
############################
Steps:
1. Detected vs Solved
2. Detected/Solved vs Suspects
3. Suspects vs Male/Female/Non_German
4. Distribution
5. Funny Rankings for the end (inhabitants from provinces) -> filter out the 6 top categories (first letter in key)
############################
############################

Once the data is imported, tidied and transformed the analysis can start. To first get a general impression over the data that we have, we
used the standard plot command, that creates a simple chart for every column. The data frame that we use for all of the next instructions
is the one with only the values for whole Germany. We first want to find correlations in there and then check for them in the single
provinces as well. \n
To make the first general data analysis more readable, we need to remove the outliers in the data. All the data in this data set is bound to the 
number of detected cases, so if the number of detected cases is 0 all other columns will also be 0 and the other way around, if there is a high
number of detected cases, the possibility of high numbers in the other columns is pretty high (Those cases will be analyzed later on). \n
To now find the number of detected cases where there are the most cases, we find the mean of detected cases. To do that we use the summary
function of R.

```{r Find_Cases_Mean}
summary(germanCaptureSet$detected_cases)
# TODO: Mean = average.
```

Now we just filter out all rows where the number of detected cases is larger than 7172. For now we are also not interested in the rows where there
are no cases, so we also filter out those.

```{r Filter_For_Number}
germanNonZeroSet <- filter(germanCaptureSet, detected_cases > 0 & detected_cases <= 7172)
```

To now get a general view on how the data is structured across the data frame we use the plot command, that creates simple plots for every
column in the frame.

```{r}
plot(germanNonZeroSet, main = "General Overview", col = "#000000", pch = 19)
```
### Detected vs. solved Cases
Based on these plots we analyze different correlations of data in the data frame. The first thing that we noticed was, that there seems to be a relation
between the number of detected cases and the number of solved cases. Next we enlarge this plot and filter out all rows where the number of solved cases
is larger than the number of detected cases, because those would be part of a measurement error. 
We know this because the documentation clearly states the solved cases are based on the detected cases. 

```{r Solvage_Rate}
filtered <- filter(germanNonZeroSet, detected_cases >= solved_cases)
plot(germanCaptureSet$detected_cases, germanCaptureSet$solved_cases, 
     col = "#cc0000",
     pch = 19,
     main = "Detected vs. Solved",
     xlab = "Solved Cases",
     ylab = "Detected Cases")
abline(lm(germanCaptureSet$solved_cases~germanCaptureSet$detected_cases)) #Draw better line
```


```{r}
cor(germanCaptureSet$detected_cases,germanCaptureSet$solved_cases)
```
### Cases vs Suspects
Referring back to our overview again, we also noticed that there seems to be a correlation between cases and suspects:
```{r}
ggplot(data = filtered, mapping = aes(x = detected_cases,y = suspects)) +
  geom_point() +
  geom_abline() +
  labs(x = "Detected cases", y = "Suspects")
```
This is also proven by the manual cor command: 
```{r}
cor(filtered$detected_cases, filtered$suspects)
```
We have a correlation of 88%. 
What we already know from the last chapter, there is also a correlation between detected and solved cases. 
Thats why we are going to use the solved cases now, and we keep the filtered set to keep the comparability. Rows where detected cases have an higher amount than solved cases can be considered as an error. 

Thats why we are going to remove outliers where detected cases are  1.5 times the interquartile range greater than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1). 
```{r}
#find Q1
Q1 <- quantile(filteredProvince$detected_cases, .25)
#find Q3 
Q3  <- quantile(filteredProvince$detected_cases, .75)
# calculate interquartile range using IQR()
IQR <- IQR(filteredProvince$detected_cases)

#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
provinceSample <- subset(filteredProvince, filteredProvince$detected_cases> (Q1 - 1.5*IQR) & filteredProvince$detected_cases< (Q3 + 1.5*IQR))

```

Outliers removed! 
```{r}
max(provinceSample$detected_cases)
```
Removing values is not always a good idea but we noticed that the residuals were not equal, thats why were restricting our province values to the Q2 and Q3 values which are at least representing all values between 25% and 75% percent of the filteredProvince dataframe now. What we can do later on is try to compare the prediction with an actual value of the filteredProvince check to see the accuracy. 

```{r}
cor(provinceSample$solved_cases, provinceSample$suspects)
```

Since the correlation is 96 percent, we can try to predict values with a linear model:
What we want to know is: Is the amount of solved cases affected by the amount of suspects? 
And later on: Is there a difference between the provinces? 
First step: Generate linear model using lm():
```{r}
(reg <- lm(provinceSample$solved_cases~provinceSample$suspects))
summary(reg)
```
What do we get? 
```{r}
hist(reg$residuals)
```
What tells us this?
If the amount of solved cases is zero, the amount  of suspects is 0,6. With every increase of solved cases by one, the amount of suspects increases by 0,91. This can be compared with a true value from the filteredProvince set now: To see how accurate that is we are taking the max() value: 
```{r}
(max <- max(filteredProvince$solved_cases))
```
Therefore, 75.850 x 0,913324 =69.275,6254 is the predicted amount of suspects. The actual value is: 
```{r}
(value <- filteredProvince$suspects[filteredProvince$solved_cases == max(filteredProvince$solved_cases)])
```
So how much deviation do we have? 
```{r}
# in percentage
(1 - (max/value))*100
```

```{r}
plot(reg)
```



```{r}
plot(provinceCaptureSet$detected_cases, provinceCa$province)
boxplot(germanNonZeroSet$detected_cases)
summary(provinceCaptureSet)
summary(provinceSample)

```





```{r}
plot(filtered$solved_cases, filtered$suspects, 
     col = "#cc0000",
     pch = 19)
abline(lm(filtered$suspects~filtered$solved_cases))
```


```{r}
plot(filtered$detected_cases, filtered$solved_cases/filtered$detected_cases, 
     col = "#cc0000",
     pch = 19,
     main = "Solvage Rate vs. Detected Cases", 
     ylab = "Solvage Rate (in percent)",
     xlab = "Detected Cases")
```

Note: Points underneath the line are suspected many times
Note: One case -> one person that did it (every one in the case gets its own law suite)
```{r Suspects_Per_Case}
par(mfrow=c(3,1))
filtered <- filter(germanNonZeroSet, detected_cases >= solved_cases)
plot(filtered$detected_cases, filtered$suspects, 
     xlim = c(0,8000),
     ylim = c(0,8000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Detected Case",
     xlab = "Detected Cases",
     ylab = "Suspects")
abline(coef = c(0,1))

plot(filtered$solved_cases, filtered$suspects, 
     xlim = c(0,8000),
     ylim = c(0,8000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Solved Cases",
     ylab = "Suspects")
abline(coef = c(0,1))
plot(filtered$solved_cases, filtered$suspects, 
     xlim = c(0,8000),
     ylim = c(0,8000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Solved Cases",
     ylab = "Suspects")
abline(coef = c(0,1))
par(mfrow=c(1,1))
```

```{r}
par(mfrow=c(3,2))
plot(filtered$distribution_under_20K, filtered$detected_cases, 
     xlim = c(0, 7000),
     ylim = c(0, 7000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Cases in City (under 20K inhibitants)",
     ylab = "Detected Cases")
abline(coef = c(0,1))
plot(filtered$distribution_20K_to_100K, filtered$detected_cases, 
     xlim = c(0, 7000),
     ylim = c(0, 7000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Cases in City (20k - 100K inhibitants)",
     ylab = "Detected Cases")
abline(coef = c(0,1))
plot(filtered$distribution_100K_to_500K, filtered$detected_cases, 
     xlim = c(0, 7000),
     ylim = c(0, 7000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Cases in City (100k - 500K inhibitants)",
     ylab = "Detected Cases")
abline(coef = c(0,1))
plot(filtered$distribution_over_500K, filtered$detected_cases, 
     xlim = c(0, 7000),
     ylim = c(0, 7000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Cases in City (over 500K inhibitants)",
     ylab = "Detected Cases")
abline(coef = c(0,1))
plot(filtered$distribution_unknown, filtered$detected_cases, 
     xlim = c(0, 7000),
     ylim = c(0, 7000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Cases in City (unknown inhibitants)",
     ylab = "Detected Cases")
abline(coef = c(0,1))
plot(filtered$distribution_unknown, filtered$detected_cases, 
     xlim = c(0, 7000),
     ylim = c(0, 7000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Cases in City (unknown inhibitants)",
     ylab = "Detected Cases")
par(mfrow=c(1,1))
```

```{r}
par(mfrow=c(3,2))
plot(germanNonZeroSet$suspects, germanNonZeroSet$male,
     xlim=c(0,8000),
     ylim=c(0,8000),
     col="#cc0000",
     pch = 19,
     main = "Suspects vs Male",
     xlab = "Suspects",
     ylab = "Male Suspects")
abline(coef = c(0,1))
plot(germanNonZeroSet$solved_cases, germanNonZeroSet$male,
     xlim=c(0,8000),
     ylim=c(0,8000),
     col="#cc0000",
     pch = 19,
     main = "Suspects vs Male",
     xlab = "Suspects",
     ylab = "Male Suspects")
abline(coef = c(0,1))
plot(germanNonZeroSet$suspects, germanNonZeroSet$female,
     xlim=c(0,8000),
     ylim=c(0,8000),
     col="#cc0000",
     pch = 19,
     main = "Suspects vs Female",
     xlab = "Suspects",
     ylab = "Female Suspects")
abline(coef = c(0,1))
plot(germanNonZeroSet$suspects, germanNonZeroSet$non_german,
     xlim=c(0,8000),
     ylim=c(0,8000),
     col="#cc0000",
     pch = 19,
     main = "Suspects vs Non German",
     xlab = "Suspects",
     ylab = "Non German Suspects")
abline(coef = c(0,1))
plot(germanNonZeroSet$male, germanNonZeroSet$non_german,
     xlim=c(0,8000),
     ylim=c(0,8000),
     col="#cc0000",
     pch = 19,
     main = "Suspects vs Male",
     xlab = "Suspects",
     ylab = "Male Suspects")
abline(coef = c(0,1))
plot(germanNonZeroSet$suspects, germanNonZeroSet$male,
     xlim=c(0,8000),
     ylim=c(0,8000),
     col="#cc0000",
     pch = 19,
     main = "Suspects vs Male",
     xlab = "Suspects",
     ylab = "Male Suspects")
abline(coef = c(0,1))
par(mfrow=c(1,1))
```

```{r}
reg <- lm(germanNonZeroSet$male~germanNonZeroSet$suspects)
summary(reg)
cor(germanNonZeroSet$suspects, germanNonZeroSet$male)
```




```{r}
par(mfrow=c(2,2))
plot(germanCaptureSet$suspects, germanCaptureSet$male, main = "Something", ylab = "Suspects", xlab = "Male")
plot(germanCaptureSet$suspects, germanCaptureSet$female, main = "Something", ylab = "Suspects", xlab = "Female")
plot(germanCaptureSet$suspects, germanCaptureSet$non_german, main = "Something", ylab = "Suspects", xlab = "Non German")
plot(germanCaptureSet$suspects, germanCaptureSet$solved_cases, xlim=c(0,150000), ylim=c(0,150000), main = "Something", ylab = "Suspects", xlab = "Cases")

par(mfrow=c(1,1))
```

```{r}
filter(merge(germanCaptureSet, crimeOverview, by="key"), suspects/detected_cases >= 0.9 & suspects/detected_cases <= 1.1)
```


```{r}
filter(crimeOverview, key == "731800")
```


```{r}
ggplot(
  data = filter(germanCaptureSet, detected_cases/inhabitants > 0.001),
  mapping = aes(detected_cases/inhabitants, solved_cases/detected_cases)) +
  #geom_point() +
  #geom_smooth(method="lm")
  geom_text(mapping = aes(label = key))
```


## Model 




