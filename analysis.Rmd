---
title: "BKA Kriminalstatistik 2019"
author: "Peter von Bodelschwingh, Joshua Gawenda"
date: "09 01 2020"
output:
  html_document:
    df_print: paged
  pdf: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("sjmisc")
#install.packages("mapproj")
#install.packages("geojsonio")
#install.packages("gridExtra")
#install.packages("car")
library(readxl)
library(dplyr)
library(sjmisc)
library(tidyverse)
library(readxl)
library(GDINA)
library(geojsonio)
library(gridExtra)
library(car)
library(broom)
```

# A data analysis

## Introduction

In this data analysis we are going to take a look at the German crime statistics from the year 2019, the special version we are looking at also
shows the crime distribution about different sized cities. The data is divided in all 16 German provinces and into a general section for the whole
country. <br/>
There are also multiple files that help to determine what can be deducted from a data analysis of this set and what is out of scope. <br/>
The data set is taken from the  "Bundeskriminalamt" - the leading institution of crime detection in Germany. <br/>
The original data set can be found here: 
[BKA Dataset](https://www.bka.de/DE/AktuelleInformationen/StatistikenLagebilder/PolizeilicheKriminalstatistik/PKS2019/PKSTabellen/LandFalltabellen/landFalltabellen.html) <br/>
This data set only shows absolute numbers for all provinces in Germany, so to make them statistics comparable over all the different provinces we
also included another data set, that shows the number of inhabitants for every province and the whole country. <br/>
This data set can be found here: 
[German Inhabitants](https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.html) <br/>
Later you will see that we imported on more data set. This one is generated out of a PDF from the documentation for the first data set and therefore
is not available online. It holds information about the so called sum keys of the main data set and is later used to decouple
the main data set.

### Sum Keys

To understand a big part of the analysis you have to understand what sum keys are, this is what we are going to describe in this chapter. <br/>
A key in the data set represents one type of crime, every crime has its own distinct key. Some crimes are just a composite of multiple other
crimes, their key is then called a sum key, so every non sum key (capture key), has no children and only a parent crime. <br/> 
A key is made out of six digits, where every digit is read as a single one from left to right. In general every key that starts with the same digits 
is part of the same sum key, the sum key is then identified with the same starting digits followed by zeros. However this is not true in every case, 
some keys that end with zero have no children and therefore are no sum keys, the sum key for those keys is in most cases represented by a "*"-sign. <br/>
One general section of crimes is therefore represented by only the first digit. There are also two whole sections of keys that are only made out of
sum keys and are used by the police to study different fields of crimes.

## Import

The first step to every data analysis is to import the data into your project. The data we chose is available as an excel file so we use the 
package "readxl" to import the sets from excel. For the import to work properly we had to remove some unnecessary header lines in the main data
set, we did that by specifying a "range" in the read_excel command. <br/>
The data set about the sum keys was created by us, therefore every optimization for importing where made while creating the set. <br/> 
Finally we also import the data set for the inhabitants, since this data set is not part of the main analysis, we decided to also edit it in
excel to optimize it for the import and remove data that we do not need for the analysis.
 
```{R Import}
originalSet <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Data",range = "A8:U18777") 
sumKeysImport <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Sumkeys")
germanInhabitants <- read_excel("DataSets/02-bundeslaender.xlsx", sheet = "inhabitants")
```

### Renaming

After the import, the names of the original data set got lost and are not descriptive anymore. To assure that the usage later on is more understandable
we renamed the data set.

```{r Renaming}
originalSet <- originalSet %>% 
                rename(
                  key = '1',
                  crime = '2',
                  province = '3',
                  detected_cases = '4',
                  detected_cases_percent = '5',
                  not_executed_cases = '6',
                  not_executed_cases_percent = '7',
                  distribution_under_20K = '8',
                  distribution_20K_to_100K = '9',
                  distribution_100K_to_500K = '10',
                  distribution_over_500K = '11',
                  distribution_unknown = '12',
                  gun_threatened = '13',
                  gun_shot = '14',
                  solved_cases = '15',
                  solved_cases_percent = '16',
                  suspects = '17',
                  male = '18',
                  female = '19',
                  non_german ='20',
                  non_german_percent = '21'
                  )
```
 
As a last step of importing, we can merge the inhabitants data frame into the original data frame for the analysis. This would allow to use the
number of inhabitants easily in any calculation from here on.

```{r Merge_Inhabitants}
originalSet <- merge(originalSet, select(germanInhabitants, "province", "inhabitants"), by = "province") %>% arrange(key)
head(originalSet)
```

## Tidy

In the next step we check for missing values and also split the capture crime keys from the sum keys in the secondary data set.

### Check non NA

If the data set contains empty values they are marked as NA and have to be handled somehow. However, this data set has no such values as shown in the
next code chunk. Here we filter for NA values in any column and check if the row count is 0 in the end.

```{r NA_Check}
nrow(filter_at(originalSet, 1:21, any_vars(is.na(.)))) == 0
```

### Decouple

Like describe in the "Sum Key" section, the original data set is a mix of sum keys and capture keys. In order to not count twice in some operations
we remove the sum keys from the capture keys, so the main analysis can focus on the capture keys. <br/>
The sum keys set that we imported has two columns, one for the key and one with the value "Y" for every capture key and a "N" for every sum key. The 
two key sections that are only made out of sum keys are not reported in the set, so can be handled as "NA".

```{r Filtering_Keys}
captureKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key))
```

To check, that the keys after the filter are all correct, we created a small algorithm that checks for sum keys based on the the rules described in
the "Sum Keys" chapter. The code takes a while to run, that is why we saved the results after the first run. 

```{r Manual_Sum_Key_Check}
#filteredKeys <- c()
#for (key1 in captureKeys$key) {
#  for (key2 in captureKeys$key) {
#    if (!key2 %in% filteredKeys && key1 != key2) {
#      if (regexpr(paste("^", str_remove(key2, "0+$"), sep = ""), key1)[1] == 1) {
#        filteredKeys <- append(filteredKeys, key2)
#      }
#    }
#  }
#}

filteredKeys <- c("133000","141100","231200","305000","310000","315000","325000","326000","335000","340000","345000","350000","390000","435000","436000","620010","655010","670010","670020","670030")
```

Since some capture keys can end on 0 we had to check the resulting values manually for wrong sorted sum keys.
After that we found, that the keys "141100", "231200" and "133000" were marked as capture keys, but are sum keys,
so these keys have to be removed from the capture keys.

```{r Correct_Key_Filter}
captureKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key) & !(key %in% c("141100","231200","133000")))
```

### Filter final data frames

To be able to filter all crimes for their main topic  we added a category column to the data frame that holds the name of the first some key for the
crime.

```{r Category, warning=FALSE}
crimeCategories <-
  originalSet %>% filter(grepl("[0-9]0{5}", key)) %>%
  select(key, crime) %>%
  unique() %>%
  mutate(key = str_replace_all(key, "00000", ""))

originalSet <- originalSet %>%
  mutate(category = substring(key, 1, 1)) %>%
  select(category, !category) %>%
  mutate(category = ifelse(category %in% crimeCategories$key,
                           crimeCategories[as.integer(category) + 1, 2],
                           "Other"))
```

## Transform

In the final step before visualizing the data we need to check all the types of the values in each column and change them to a type that is usable
for our analysis. We can also remove and create new Columns if necessary. In the end we are also splitting the original data frame up based on the
provinces and the sum keys determined before.

### Transforming data types

In this step we first check for the different data types we have in every column. We expect to have characters as the key, a factor for the provinces
and categories, every other column should be a number.

```{r Type_Check}
sapply(originalSet, class)
```

Except for the expected factors all values have the type expected, the next step is to introduce these.

```{r Factorize_Province}
originalSet$province <- as.factor(originalSet$province) 
originalSet$category <- as.factor(originalSet$category)

sapply(originalSet, class)
```

```{r include=FALSE}
c1 <- levels(originalSet$category)[1]
c2 <- levels(originalSet$category)[2]
c3 <- levels(originalSet$category)[3]
c4 <- levels(originalSet$category)[4]
c5 <- levels(originalSet$category)[5]
c6 <- levels(originalSet$category)[6]
c7 <- levels(originalSet$category)[7]
c8 <- levels(originalSet$category)[8]
c9 <- levels(originalSet$category)[9]
```

### Enter Province ID

To be able to use the provinces later in a map we have to give all the provinces an id.
Since it is important that each province has the same id as the provinces used in the geojson dataframe, the id has to be the same. Thats why we are adding the id manually and not with a formula. 

```{r Province_Id}
province_ids <- data.frame(
  province =
    c(
      "Baden-Württemberg",
      "Bayern",
      "Berlin",
      "Brandenburg",
      "Bremen",
      "Hamburg",
      "Hessen",
      "Mecklenburg-Vorpommern",
      "Niedersachsen",
      "Nordrhein-Westfalen",
      "Rheinland-Pfalz",
      "Saarland",
      "Sachsen-Anhalt",
      "Sachsen",
      "Schleswig-Holstein",
      "Thüringen",
      "Bundesrepublik Deutschland"
    ),
  province_id =
    c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17")
)
originalSet <- originalSet %>% merge(province_ids, by = "province")
```

### Remove calculated columns

Some columns in the data set are just calculated from the given data, so to remove the possibility of a calculation error, we remove these columns
and add them on our own, once they are needed in the analysis.

```{r Select_Useful_Columns}
originalSet <- originalSet %>% select(key, 
                                      category,
                                      crime,
                                      province, 
                                      province_id,
                                      inhabitants, 
                                      detected_cases, 
                                      solved_cases, 
                                      not_executed_cases, 
                                      suspects, 
                                      male, 
                                      female, 
                                      non_german, 
                                      gun_threatened, 
                                      gun_shot, 
                                      distribution_under_20K, 
                                      distribution_20K_to_100K, 
                                      distribution_100K_to_500K, 
                                      distribution_over_500K, 
                                      distribution_unknown)
```

### Split up data

This could also be part of the tidy step, but to reduce the amount of duplicate code for all checks, we split the data at the end. Therefore, we
merge the key data frame together with the original data set to separate the capture keys into a separate data frame.

```{r Split_Original_In_Sum_Capture}
captureValues <- merge(originalSet, captureKeys) %>% select(!is_sum_key)
```

With the sum keys now separated, we also divide both data frames into the values for the provinces and the values for whole Germany.

```{r Separate_Province}
germanCaptureSet <- captureValues %>%
  filter(grepl("Bundesrepublik Deutschland", province))

provinceCaptureSet <- captureValues %>%
  filter(!grepl("Bundesrepublik Deutschland", province))
```

To check that the sum keys from before where separated correct we check that the sum of all detected cases in the German set is equal to the sum 
of the detected cases in the province set.

```{r Sum_Key_Check}
sum(germanCaptureSet$detected_cases) == sum(provinceCaptureSet$detected_cases)
```

## Prepare Map

Before our analysis can start, we want to display some data on a map, with all German provinces. In this step we prepare the empty map, so that it can
easily be used later on.

```{r Map_Preparation, message=FALSE}
province_json <- geojson_read("provinces.json", what = "sp")
empty_map <- tidy(province_json, area = "code") %>%
  mutate(province_id = id)

ggplot(empty_map, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", color = "grey") +
  theme_void() +
  coord_map()
```

We also created a function to display a map for a data set.

```{r Display_Function}
displayMap <- function(data, FUN, title, title_fill) {
  data <- data  %>%
    select(province_id, fill) %>%
    mutate(province_id = as.integer(province_id))
  
  data <- aggregate(data, list(data$province_id), FUN = FUN) %>%
    select(!province_id) %>%
    rename(province_id = "Group.1") %>%
    mutate(province_id = as.character(province_id))
  
  map_data = empty_map %>% left_join(., data, by = "province_id")
  
  ggplot(map_data, aes(long, lat, group = group, fill = fill)) +
    geom_polygon(color = "black") +
    geom_polygon(data = filter(map_data, province_id == 3),
                 color = "black") +
    geom_polygon(data = filter(map_data, province_id == 5),
                 color = "black") +
    theme_void() +
    coord_map() +
    labs(title = title, fill = title_fill)
}
```


```{r, echo=FALSE}
options(scipen=999)
```
    
## Analyse 

Once the data is imported, tidied and transformed the analysis can start. To first get a general impression over the data that we have, we
used the standard plot command, that creates a plot for every variable in the data frame. We first only look at the values for whole
Germany to keep the basic analysis simple. <br/>
To make the first general data analysis more readable, we need to remove the outliers in the data. The value of the detected cases on our data set
determines the number of observations, if we remove the outliers there, the whole data set should be more readable. <br/>
Now lets look at the summary for the detected cases.

```{r Find_Cases_Mean}
summary(germanCaptureSet$detected_cases)
```

The easiest way to filter the high values is now to filter out every value above the mean. For now we are also not interested in the rows where there
are no cases, so we also filter out those. After removing the outliers we can take a look the the plots created. 

```{r Filter_For_Number, fig.height=8, fig.width=15}
cutSet <- filter(germanCaptureSet, detected_cases > 0 & detected_cases <= 7172)

plot(cutSet, main = "General Overview", col = "#000000", pch = 19)
```

Based on the plots displayed there we figured, that we want to focus on the factors that influence the solvage of crime cases and explore the data
based on detected cases, solved cases, suspects and the crime distribution.

### Detected Cases vs Solved Cases

The first thing that we noticed was, that there seems to be a relation between the number of detected cases and the number of solved cases. 
Next we enlarge this plot and filter out all rows where the number of solved cases is larger than the number of detected cases. Those would 
be part of a measurement error, since every crime can only be solved once. We again also remove those cases where the number of detected cases is 0.

```{r Solvage_Rate, fig.height=8, fig.width=15}
filtered <- filter(germanCaptureSet, detected_cases >= solved_cases & detected_cases > 0)

ggplot(filtered, aes(detected_cases, solved_cases, color = category)) +
  geom_point(show.legend = T) +
  geom_smooth(method = "lm", fullrange = T, show.legend = F) +
  geom_abline() +
  labs(title = "Detected vs. Solved", y = "Solved Cases", x ="Detected Cases")
```

This plot shows the relation between detected and solved cases for each major crime category.
The black line displayed in this plot shows the optimal line, where the number of detected cases is equal to the number of solved cases, so those crimes
where every case was solved. <br/>
The colored lines displayed show the estimated linear correlation between solved and detected cases. Some of the lines are extended to the right to
even be visible, the light blue line has only crimes with few cases, so most of the lines are only predictions. Lets now take a closer look at the 
different solvage rates.

```{r Adding_Solvage_Rate, inclue=FALSE}
germanCaptureSet <- germanCaptureSet %>% mutate(solvage_rate = solved_cases/detected_cases)
provinceCaptureSet <- provinceCaptureSet %>% mutate(solvage_rate = solved_cases/detected_cases)
```

```{r Calc_Solvage_Rate, fig.height=8, fig.width=15}
filtered <- filtered %>%
  mutate(solvage_rate = solved_cases / detected_cases * 100)

ggplot(filtered, aes(category, solvage_rate, fill = category)) +
  geom_boxplot() +
  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +
  labs(title = "Solvage Rate", x = "", y = "Rate in Percent")
```

Before getting into the analysis of the graphs, lets also look at the numbers associated with the solvage rate.

```{r Solvage_Rate_Numbers}
# Crimes associated with theft
descr(filter(filtered, category == c4 | category == c5)$solvage_rate)
# General
descr(filtered$solvage_rate)
```

The boxplots show a high variation in the solvage rate of cases based on their category. In general the solvage rate is very high,
and the mean lies at 69% solvage. There are just two exceptions, which are those crimes associated with theft, their mean solvage rate lies around 36%.
There is also one more crime category that stands out, murder. This has an extremely low inner quartile range which means, that the solvage rate for
this crime is very consistent. <br/>
Together with the murder, there are two more categories, that have a high mean solvage rate and a low inner quartile range. These categories are special 
cases in the data set and there is possibly a specific reasons behind the relatively consistent and high solvage rate. <br/> 
<br/>
Now that we analyzed the solvage rate for the different crime categories for whole Germany, we can take a quick look at the rates for the different 
provinces. To keep this part short, we just used the overall mean solvage rate for every province instead of separating it into to different categories.


```{r Solvage_Rate_Map, fig.height=8, fig.width=12}
province_filtered <- provinceCaptureSet %>%
  filter(detected_cases >= solved_cases & detected_cases > 0) %>%
  mutate(fill = solved_cases / detected_cases * 100)

displayMap(province_filtered, mean, "Province Mean Solvage Rate", "Rate in percent")
```

When looking on this map, there are five relatively dark provinces on the map, which means, that they have the lowest solvage
rate in Germany. Four out of these provinces are the smallest provinces in Germany while the fifth has no connection in size to the others. <br/>
To get a better picture on which provinces are the most dangerous, based on their mean solvage rate and their crimes per inhabitant, we create a
second map that shows the crimes per 10.000 inhabitants.

```{r Crime_Rate_Map, fig.height=8, fig.width=12}
province_filtered <- provinceCaptureSet %>%
  filter(detected_cases >= solved_cases & detected_cases > 0) %>%
  mutate(fill = detected_cases / inhabitants * 10000)

displayMap(province_filtered, sum, "Province Accumulated Crimes", "Crimes per 10.000 inhabitants")
```

And again, the three lightest spots on the map, therefore the ones with the highest crime rate, are the three smallest provinces in Germany. Also the 
other two provinces from the last map have a tint that is close to the average of the scale. <br/>
One more province has to mentioned when describing both maps, on both maps Bayern has the lowest crime rate and the highest solvage rate.

### Solved Cases vs Suspects

Now that we analyzed the aspects of the solvage rate in Germany, we want to have a look at the suspects involved in solving a case. How many suspects 
do you need to close a case? Are more suspects better for solving the case? <br/>
<br/>
Before the analysis can start there is one thing to clarify, how are the suspects counted. One thing was not clarified by the documentation, how many
culprits does a crime have. From what we know about German law we just had to assume, that every culprit gets his own law suit, so therefore if a group
of people commits a crime, every person involved will receive an entry in this statistic. Also a person will only be listed once per crime, so if 
person x committed fraud twice and theft once, then he will appear both in the fraud and theft section only once as a suspect. This means, that a crime 
with 100% solvage rate and a lower amount of suspects than detected cases is only explainable by repead offenders. <br/>
For sum keys about the suspects the same rules apply, so a suspect that commits multiple crimes for the same sum key will only be counted once, so 
summing up all capture keys contained in a sum key will often lead to a higher number of suspects than what is written in the sum key. <br/>
<br/>
Referring back to our overview again, we also noticed that there seems to be a correlation between solved cases and suspects, so lets enlarge that view.
```{r Suspects_VS_Solved, fig.height=8, fig.width=15}
ggplot(data = germanCaptureSet, mapping = aes(x = solved_cases,y = suspects)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(title = "Suspects vs. Solved Cases", x = "Solved cases", y = "Suspects")
```

Since we want to calculate the suspects rate later on, we have to remove all rows where no crimes have been detected and those rows where no suspects 
could be found because a division would lead to NA values. <br/>
We now also divided the next plot into the different categories to look closer on its effect for the suspect rate.

```{r Individual_Suspect_Rate, fig.height=8, fig.width=15}
solvedCasesFiltered <- filter(germanCaptureSet, detected_cases > 0, suspects > 0)

ggplot(solvedCasesFiltered, aes(solved_cases, suspects, color = category)) +
  geom_point() +
  geom_smooth(method = "lm", show.legend = F) +
  geom_abline() +
  labs(title = "Suspects vs. Solved", x = "Solved Cases", y ="Suspects")
```

As can be seen, most of the values are below 100.000 solved cases. Afterwards the curves start deviating which is caused by a few values only. Therefore, we have to take a look at how the population looks like using a boxplot: 

```{r Suspect_Boxplots, fig.height=8, fig.width=15}
solvedCasesFiltered <- solvedCasesFiltered %>%
  mutate(suspects_rate = suspects / solved_cases)

ggplot(solvedCasesFiltered,
       aes(category, suspects_rate, fill = category)) +
  geom_boxplot() +
  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +
  labs(title = "Suspects Rate", x = "Rate by category", y = "Suspects rate")
```

All of the inner quartile ranges are small, the most values are between 1 and 1.6 and most of the outliers are higher than 2. There are few
outliers below 1, only for crimes associated with fraud, the median is below 1. Paired with the relatively high solvage rate
this could indicate a higher amount of repead offenders for those kind of crimes. <br/>
There is only one way to determine repead offenders with a high certainty and that is to only look at crimes with 100% solvage rate and a suspect rate 
below 100%. The lower the suspect rate, the higher the amount of repead offenders.

```{r Repead_Offenders}
repead_offenders <- solvedCasesFiltered %>%
  filter(solvage_rate == 1, suspects_rate < 1)

repead_offenders %>% select(crime, suspects_rate, category) %>%
  arrange(suspects_rate)
```

#### Outliers analysis

Going back to the graph we want to take a closer look at the outliers, lets look at every crime for the category "Sonstige Straftatbestände" that is
above marked as an outlier in the boxplot.

```{r Outliers}
outliers <- solvedCasesFiltered %>%
  filter(category == c3, suspects_rate >= 2)

outliers %>% select(crime, suspects_rate) %>%
  arrange(desc(suspects_rate))
```

Looking at the first four crimes with the highest suspect rate, it occurs that they can only be committed by more than one suspects. So it is possible
that in order to find the one person leading the group, everyone was suspected.

#### Linear model

After looking at the outliers, we can try to create a linear model, since the first graph of this chapter seems to hint at a positive correlation
of the variables. We start by looking at the mathematical correlation of the variables.

```{r Correlation_Check}
germanCasesSet <- germanCaptureSet %>% filter(detected_cases > 0, solved_cases > 0)
cor(germanCasesSet$solved_cases, germanCasesSet$suspects)
```

Now there seems to be a almost 99% correlation, so we proceed with creating and summarizing the linear model.

```{r Linear_Model}
reg <- lm(solved_cases ~ suspects, data = germanCasesSet)

summary(reg)
```

The outliers that we determined before are clearly visible due to the high deviation in the minimum and maximum values of the residuals compared with the quartiles. Furthermore what we can see is that the models standard errors are low, but the residual standard error is high. This is due to the high outliers mentioned already. That is why we now create a model where we filter out the outliers before.

```{r Adjusted_Model}
Q1 <- quantile(germanCasesSet$solved_cases, .25)
Q3  <- quantile(germanCasesSet$solved_cases, .75)
IQR <- IQR(germanCasesSet$solved_cases)

modelSet <- germanCasesSet %>%subset(solved_cases> (Q1 - 1.5 * IQR) & solved_cases< (Q3 + 1.5 * IQR))

reg <- lm(solved_cases ~ suspects,data = modelSet)
summary(reg)
```

We can see that, when solved cases are zero, the amount of suspects is about 11 and with each increase of solved cases by one, the amount of suspects 
increases by 0,92. With about 10 the standard error at the intercept is relatively high, and the null hypothesis has a high percentage to be true. 
So when interpreting the model the error for a low amount of solved cases is relatively high. For our analysis however this does not reject our
regression model. <br/>
The second variable in the model has a low possibility for the null hypothesis to be true, which means, that Suspects are statistically significant 
for a change of solved cases. <br/>
Also, the residual standard error follows the impression of the SSE: It is very high, although we have a high amount of freedom degrees
(595 rows - 2 variables in the model = 593 DF). Looking at the R squared tells us here that, although the RSE and the SSE seem to be very high, the model 
determines a good fit: 0,9534. Therefore, 95 percent of the variation in the data is explained by the model. <br/>
In conclusion, the model has a good fit, but it needs to be taken into account that the amount of DF is very high, therefore the variation can be and is 
influenced by other variables.  <br/>
In the next step, we will look at the residuals how they spread.  <br/>
Residuals are the difference of the observed y values (solved cases) and the predicted values of y from our model. If the prediction is higher than the 
observed value, than the residual is going to be negative. If predicted and observed value are equal the residual is zero. <br/>
To get a better understanding of the variation of residuals we look at a histogram for this.

```{r Hist_Residuals, fig.height=8, fig.width=15}
hist(reg$residuals,
     col = "#cc0000",
     main = "Residual spread",
     xlab = "Residuals")
```

This plot show, that we have a high number of low negative residuals, that are canceled out by a lower number of high positive residuals. This is
not a problem, but has to be mentioned for the further analysis.

```{r Reg_Plot, fig.height=8, fig.width=15}
par(mfrow=c(2,2))
plot(reg)
```

```{r include=FALSE}
par(mfrow=c(1,1))
```

Lets look at the plots one by one: <br/>
<br/>
Residuals vs Fitted <br/>
As we can see, the red line follows the dotted line because of the equal spread of positive and negative residuals. There is no pattern visible
that could hint towards a different relation than a linear regression. Also what was shown before in the histogram is also visible here, there
are many residuals directly below the red line and some high above. <br/>
<br/>
Normal Q Q <br/> 
We can see residuals spreading out in the upper and lower quantiles. This tells us that the spread inside the population is not equal, especially at 
the end of the line. This is due to a less observations of solved cases in a range greater than 10.000. The part of the line with the most observations
follows the dotted line very close and therefore supports this model. <br/>
<br/>
Scale location <br/>
This plot also shows the equal spread of residuals. There are just a few outliers on the upper side, but for most of the line the points seem to be
scattered evenly and therefore proof a homoscedasticity. <br/>
<br/>
Residuals vs Leverage <br/>
There are no real cases with an extremely high influence on the model, there are only a few with a bit higher influence and like shown before, there
is a higher amount of them in the positive region. <br/>
The plots and summary of the linear model show a high accuracy, that is what we now test by predicting a value with the model and calculating the 
difference to the observed value.

```{r Predict_Values, fig.height=8, fig.width=15}
actual <- germanCasesSet %>% select(solved_cases,suspects) 

p <-
  data.frame(
    suspects = actual$suspects
  )
p <- as.data.frame(p)


predicted <- predict(reg, newdata = p)
p["predicted"] <- predicted
p["actual"] <- actual$solved_cases

p <- p %>% mutate(offsetAdjusted = (actual - (predicted - 10)) / actual * 100, 
                  offset = (actual - predicted) / actual * 100)

summary(p$offset)
```

Looking at the summary of the offset we can see, that there is a big difference in the mean and median offset, therefore there are large outliers
that shift the mean. In general the accuracy of the predictions made is around 86% because of the median of 14% offset from the real value. <br/>
The model is not that accurate since the mean is way below the median and the outliers are at over 1500% offset. Going back to the summary of the
regression model, we noticed that the standard error for the intercept is almost as high as the intercept itself and the null hypothesis has a high
chance of being accepted. That is why we also created a offset value where we subtract the standard value of the intercept from the prediction value
to reduce the impact the intercept has on the overall model.

```{r Adjusted_Predict}
summary(p$offsetAdjusted)
```

Now here we can see, that the mean and median drastically moved towards 0 and the outliers also got reduced by a lot. This model shows a higher 
accuracy in its predictions than the first one, the mean is at 17% offset from the real value and the center of the data has a prediction 
accuracy of 96%. That said the model still has some bigger outliers, but in general the adjusted regression model is relatively accurate and is
therefore accepted.

### Suspects and how they are structured

Now that we analyzed how suspects affect the outcome of an investigation, lets look into how the suspects are separated on their own. There are three
variables that describe the suspects more detailed: Male, female and non German suspects. Male and female suspects together add up to the total number 
of suspects, while the non German suspects are just a part of it. <br/>
Lets start with looking at the female and male distribution on crimes.

```{r Male_Female_Sus, fig.height=8, fig.width=15}
ggplot(germanCaptureSet) +
  geom_point(aes(suspects, male), color = "blue") +
  geom_point(aes(suspects, female), color = "red") +
  geom_abline() + 
  labs(title = "Male and Female Suspect Rate", x = "Number of all Suspects", y = "Male(blue) / Female(red)")
```

The graph above shows the blue dots as the male suspects, while the red dots represent the female. The black line 
shows the point where there would be a 100% majority of one sex of the suspects. <br/>
The plot shows very clearly, that the majority of suspects are male. To further analyze this aspect we introduce the "male coefficient" that show
the percentage of male suspects, the females would then be whats missing to the 100%.

```{r Male_Coef, fig.height=8, fig.width=15}
maleCoef <- germanCaptureSet %>% filter(suspects > 0) %>% mutate(male_coef = male/suspects)

hist(maleCoef$male_coef, 
     breaks = 20,
     freq = F,
     col = "#cc0000",
     main = "Male coefficient in all suspects",
     xlab = "Male percentage")

curve(dnorm(x, mean = mean(maleCoef$male_coef), sd = sd(maleCoef$male_coef)),
      col = "grey",
      lwd = 2,
      add = T)

lines(density(maleCoef$male_coef, adjust = 1), col = "blue", lwd = 2)
lines(density(maleCoef$male_coef, adjust = 3), col = "lightblue", lwd = 2)
rug(maleCoef$male_coef, lwd = 1, col = "lightblue")
```

This histogram shows the distribution of the male coefficient in the data frame, it is overlayed with a normal distribution (gray line) and a line
representing the density of the data (dark blue = raw, light blue = smoothed). All the curves follow the chart pretty close and display a clear cluster
around the mean value. <br/>
This plot supports the view of the first chart, that there are a lot more male suspects than female, most cases have around 83% male suspects.

```{r Described_Male_Coef}
descr(maleCoef$male_coef)
```

Because the mean and the median are very close, both values give a very accurate view of the reality. This data however does not show how many crimes 
are committed my men, since they could also be wrongly accused. <br/>
To find the male coefficient for crimes that where really committed by men, we have to look at the values that have the same amount of suspects as the
amount of solved cases, because this means that every case got a single suspect and therefore no wrong accusations.

```{r Rightful_Accused, fig.height=8, fig.width=15}
par(mfrow=c(1,2))
maleCoefFiltered <- maleCoef %>% filter(solved_cases==suspects)

boxplot(maleCoef$male_coef,
        col = "#cc0000",
        main = "All suspected",
        ylab = "Male percentage",
        xlab = paste(nrow(maleCoef)," rows"))
boxplot(maleCoefFiltered$male_coef,
        col = "#cc0000",
        main = "Rightful suspected",
        ylab = "Male percentage",
        xlab = paste(nrow(maleCoefFiltered)," rows"))
```

```{r include=FALSE}
par(mfrow=c(1,1))
```

Comparing the two different graphs, they have many points in common and are not that different from one another. Both show a high rate of crimes
with more male suspects and only outliers for crimes with a high female suspection rate. The inner quartile range of both plots is almost the
same height and the lower whisker is only a bit smaller on the right plot. <br/>
Effectively the right plot is similar to the left one, but shifted up by a lot, the number of males where we know for a fact that they committed is
extremely high. There are also even fewer outliers and only one or two cases where there are more women suspected that men.
<br/>
Lastly lets look at the male coefficient distributed over the different provinces of Germany.

```{r Map_Male_Coef, fig.height=8, fig.width=12}
province_filtered <- provinceCaptureSet %>%
  filter(suspects > 0) %>%
  mutate(fill = male / suspects / 699)

displayMap(province_filtered, sum, "Province Accumulated Percentage of Male Accusations", "Male percentage")
```

To bring back the amount of non German suspects, lets also create a map for those.

```{r Map_Non_German, fig.height=8, fig.width=12}
province_filtered <- provinceCaptureSet %>%
  filter(suspects > 0) %>%
  mutate(fill = non_german / suspects / 699)

displayMap(province_filtered, sum, "Province Accumulated Percentage of Non German Accusations", "Non German percentage")
```

Both maps show a lower percentage in the eastern states of Germany and mark the same provinces with a higher percentage of suspects. The map for the
non German suspects shows a large gap between those provinces with a higher non German suspect rate and those with a lower one, it is almost halved
for a lot of provinces. Interestingly enough all eastern provinces have a low amount of non German suspects, except for Berlin which has one of the 
highest percentages. <br/>
Comparing the two maps, it is clear that those states with a higher amount of male suspects also have a high amount of non German suspects, so there
is a high chance that a non German suspect is also going to be male. However those variables can not clearly be compared in this data set, so it is
very well possible that all non German suspects are female, but the chance for that is relatively low.

### Distribution

We now know how the amount of suspects is connected to a solved case and how the suspects are split up into male and female. Now in this last analysis 
chapter we take a look on the distribution of crimes. We try to find a correlation between the distribution of crimes and their solvage. <br/>
The problem in our data set is, that we do not know the amount of solved cases per distribution, we only know the amount of detected cases, for the
further analysis we just look at the relative solvage rate based on the number of detected cases in the distribution. <br/>
To start of with this chapter we have to modify our data frames, so that the columns for the different distributions are getting reduced into two
columns, one for the distribution name as a factor and one for the corresponding value.

```{r Melting, message=FALSE}
#install.packages("reshape")
library(reshape)

ids <- c(
  "key",
  "crime",
  "category",
  "province",
  "inhabitants",
  "detected_cases",
  "solved_cases",
  "not_executed_cases",
  "suspects",
  "male",
  "female",
  "non_german",
  "gun_threatened",
  "gun_shot",
  "province_id",
  "solvage_rate"
)

meltedGerman <- melt(filter(germanCaptureSet, detected_cases > 0, solved_cases > 0), id = ids)
meltedGerman$value <- as.integer(meltedGerman$value)
meltedGerman <- meltedGerman %>% mutate(
  rel_solvage = value / detected_cases * solved_cases / detected_cases,
  relevance = value / detected_cases
) %>%
  mutate(shift = relevance - rel_solvage)

meltedProvince <- melt(filter(provinceCaptureSet, detected_cases > 0, solved_cases > 0), id = ids)
meltedProvince$value <- as.integer(meltedProvince$value)
meltedProvince <- meltedProvince %>% mutate(
  rel_solvage = value / detected_cases * solved_cases / detected_cases,
  relevance = value / detected_cases
) %>%
  mutate(shift = relevance - rel_solvage)

detach("package:reshape", unload = TRUE)

meltedGerman <-
  meltedGerman %>% rename(distribution = "variable", distribution_value = "value")
meltedProvince <-
  meltedProvince %>% rename(distribution = "variable", distribution_value = "value")
```

After melting the data sets, we also introduced new calculated variables. We calculated the relevance of a distribution, which represents the 
distributions percentage on all detected cases. The relative solvage rate is the solvage rate of the crime multiplied by the distributions 
relevance. The maximum value for the relative solvage rate therefore is the relevance. That is why we also calculated the difference of to the 
optimal value as the shift.

```{r Relevance_Solvage, fig.height=8, fig.width=15}
grid.arrange(
  ggplot(filter(meltedGerman, distribution_value > 0)) +
    geom_boxplot(aes(relevance, distribution)) +
    geom_boxplot(aes(rel_solvage, distribution), color = "red", fill = "transparent") +
    labs(
      title = "Distributions relevance and relative solvage",
      y = "",
      x = "Relevance / Relative Solvage"
    ),
  ggplot(filter(meltedGerman, distribution_value > 0)) +
    geom_boxplot(aes(shift, distribution)) +
    labs(
      title = "Distributions variance from optimal solvage",
      y = "",
      x = "Difference from optimal solvage"
    ),
  nrow = 2
)
```

On the first glance the first plot looks very busy, but it is important to overlay the different boxplots. The underlying black one displays the
distributions relevance, while the red overlay represents the relative solvage rate. The first thing to notice in these plots is, that the
distribution unknown is an outlier; it has a small relevance but an extremely high relative solvage rate, they almost overlay perfectly. When looking
at the second plot, this picture solidifies, there we can see the extremely low shift from an optimal solvage rate. <br/>
Going back to the other values, it is visible, that the relevance is more consistent for distributions from 20K to 100K and from 100K to 500K even though
the relevance itself varies much between those two. It seems like the most crimes occur in distributions from under 20K to 100K followed by the 
distribution over 500K. Based on the inner quartile range of the solvage rate it seems, that the consistency for the solvage rate increases for 
distributions with lower relevance. <br/>
Looking again at the second graph it is also visible, that the distribution from 100K to 500K has a lower shift than the other three (except for the 
unknown), all the others are very similar, but this one has a higher relative solvage rate. <br/>
There seems to be a relation between the relevance and solvage rate of a distribution, lets look closer at this.

```{r Solvage_VS_Relevance, fig.height=8, fig.width=15}
ggplot(meltedGerman, aes(relevance, rel_solvage, color = category)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(
    title = "Solvage vs. Relevance",
    x = "Relevance",
    y = "Relative Solvage in percent"
  ) +
  facet_wrap(~distribution)
```

Above we plotted the relevance and solvage for each distribution and every category and added a optimal line to every group. Going back the the boxplots
from before, the distribution unknown should have the lowest overall relevance and a high solvage rate. The facet for this distribution does not show
that, there is only one category visible with a high solvage rate, while the others are very low. So possibly all the values with a lower relevance are
clustered at the low end where the solvage rate is not visible in this view. <br/>
On the other hand the distribution 100K to 500K exactly shows what was displayed before, many values are clustered with a low relevance, but the
solvage rate in many categories is very high. The only categories with a lower solvage rate are related to theft, like described in the first chapter.
But while the solvage rate of crimes related to theft is low, they always have a relevance of under 50%. <br/>
Whats also interesting to see is, that crimes related to fraud have a general relevance of under 50% and a high solvage rate for most 
distributions, except for the distribution over 500K and also the outlier unknown. <br/>
<br/>
In general there seems to be positive relation between the relevance and relative solvage rate, that is why we now want to create a multiple regression 
based on the solvage rate, relevance and the categories, but since the distribution unknown and the distribution over 500K seems to have a different 
pattern in their correlation we decided to filter them out for the linear model. <br/>
To also be sure that the multiple regression model with the categories is a more accurate model than the one without we compare both of them first.

```{r Compare_Regression_Models}
modelData <- meltedGerman %>% filter(distribution != "distribution_unknown", distribution != "distribution_over_500K")

reg <- lm(rel_solvage~relevance+category, data = modelData)
reg2 <- lm(rel_solvage~relevance, data = modelData)

anova(reg, reg2)
```

When looking at the RSS of both models it, the RSS of the first model, the one with the multiple linear regression, is much lower and therefore more 
accurate than the other one. The next step is to assure that there is no collinearity between the different variables of the multiple linear regression
and to look at the summary for the model.

```{r Analyse_Regression_Values}
vif(reg)

summary(reg)
```

Looking at the values from the vif command, there is a low risk of collinearity in this model, so we can proceed. <br/>
The summary for the different variables affecting the linear regression shows, that the null hypothesis is not valid for most of the variables, there only
seems to be an issues with the fraud category. Also looking at the estimate for the theft category shows that their impact is larger than the other
variables and negative which displays them having a lower curve in the plot before. <br/>
The standard errors for the variables is quite low and even the residual standard error is not noticeable. All in all the R squared shows, that
over 79% of  the data variation can be explained by this regression model. <br/>
To be sure about the validity of the and power of this model, we take a look at the graphs created for the regression model.

```{r Regression_Plots, fig.height=8, fig.width=15}
par(mfrow=c(2,2))
plot(reg)
```

```{r include=FALSE}
par(mfrow=c(1,1))
```

The Residuals vs Fitted plot shows a relative even distribution of values until 0.4, after that it slowly starts drifting. In general the line still 
is relatively close to the dotted line and there is no visible pattern, so the linearity is still given. <br/>
The Normal Q-Q plot shows the same smaller issue like the plot before, most of the values follow the line pretty close, even the values in the upper
quarter only have a small variation, but the lowest quartile shows a drift in residuals. But also this is not enough to show big flaws in the model, 
some variation will always be there, when using this model however you have to be a bit more careful. <br/>
The next Scale-Location plot shows a nice even distribution of points along the red line and proofs that we have a homoscedasticity. <br/>
The last plot only shows that there are no special cases with an extremely high influence on the regression, there are three cases with a higher
influence, but not significant. <br/>
Lets now predict a value for every observation in our data set and calculate the accuracy of the prediction. To visualize this in the end we 
plot the accuracy in a boxplot afterwards.

```{r Predict_Multiple_Values, fig.height=8, fig.width=15}
actual <- modelData %>% select(rel_solvage,relevance,category) %>% filter(rel_solvage > 0)

p <-
  data.frame(
    relevance = actual$relevance,
    category = actual$category
  )
p <- as.data.frame(p)


predicted <- predict(reg, newdata = p)
p["predicted"] <- predicted
p["actual"] <- actual$rel_solvage

p <- p %>%  mutate(offset = (actual - predicted) / actual * 100)

ggplot(p, aes(category, offset, color = category)) +
  geom_boxplot() +
  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +
  labs(
    title = "Accuracy of predictions",
    x = "",
    y = "Offset from actual value in percent"
  )
```

In general the accuracy of the regression model is extremely high, all of the boxplots are around 0, but since there is a probability party to every
regression model, there are also a lot of outliers. The highest outliers are part of the theft category which makes sense, since their value also
differs a lot from the others.

```{r Prediction_Summary}
summary(p$offset)
```

The general accuracy of the predictions is around 79%, and therefore the same value as the r squared of the regression model. In general the 
regression model has a high accuracy and is therefore accepted. <br/>
<br/>
Now at the end of this chapter lets also look at how the distributions are present in the different provinces.

```{r Province_Distribution_Relevance, fig.height=8, fig.width=15}
province_filtered <- meltedProvince %>%
  filter(relevance > 0, rel_solvage <= 1)

ggplot(province_filtered, aes(relevance))  +
  geom_line(
    aes(y = ..density..),
    lwd = 1,
    stat = 'density',
    adjust = 3,
    col = "red"
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(province_filtered$relevance),
      sd = sd(province_filtered$relevance)
    ),
    lwd = 1,
    col = "blue"
  ) +
  geom_histogram(aes(y = ..density.., fill = distribution), alpha = 0.4) +
  labs(
    title = "Province Distribution Relevance",
    x = "Relevance",
    y = "Density"
  ) +
  facet_wrap( ~ province)
```

These plots show in a way the amount of larger and smaller cities of the different provinces. It is visible, that distributions over 500K are not present
in every province and wherever they are they seem to have an impact on the relevance density. Every plot has the same blue line which represents
the normal distribution of the complete data frame, while the red line shows the density curve of all distributions for the specific province. <br/>
All provinces without any distribution over 500K has a red line that closely follows the blue line, while the other provinces are shifted. All other
provinces except for the Berlin, Bremen and Hamburg are shifted to the left meaning, that they have more distributions with a lower relevance. This 
however makes sense since those provinces where there are more types of distributions, have to split all their detected cases with more distributions. <br/>
Some provinces have an equal spread of relevance for the different distributions, but for example Schleswig-Holstein, Bayern, Brandenburg and 
Rheinland-Pfalz have a higher relevance for all their distributions under 20K, even though from the height of the bars it does not seem like they have an 
extremely high amount of these distributions. <br/>
Its also interesting to see, that the distribution unknown category has a mostly equal spread over all the provinces, while Berlin does not have any
unknown distributions. To be clear, Berlin, Hamburg and Bremen are special cases since they are provinces and cities in one, but for some reason
Berlin stands out of even these three.

```{r Province_Distribution_Solvage, fig.height=8, fig.width=15}
ggplot(province_filtered, aes(rel_solvage))  +
  geom_line(
    aes(y = ..density..),
    lwd = 1,
    stat = 'density',
    adjust = 3,
    col = "red"
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(province_filtered$rel_solvage),
      sd = sd(province_filtered$rel_solvage)
    ),
    lwd = 1,
    col = "blue"
  ) +
  geom_histogram(aes(y = ..density.., fill = distribution), alpha = 0.4) +
  labs(
    title = "Province Distribution Relative Solvage",
    x = "Relative Solvage",
    y = "Density"
  ) +
  facet_wrap( ~ province)
```

We also created the same plots with the relative solvage rate instead of the relevance, with the same blue and red curve on top. The same observation
where the red line follows the blue for all provinces without distributions of over 500K can be made here. <br/>
This time the plots look more ordered after color and height, meaning that every plot starts with a high density of low solvage rates that slowly decreases
while the solvage rate increases. In general the density for the unknown distributions decreases, then it seems like the higher distributions start
decreasing their density first. At the end of every plot there is a small bump of cases with a 100% relative solvage rate. <br/>
Except for the before named three city-provinces, there are two exceptions to the observations from above, first in Sachsen the solvage rate for
the distributions over 500K has a sudden bump around the mean of the normal distribution and second in Nordrhein-Westfalen the distributions under
20K fall of very quickly, together with the distribution unknown.

### Summary

To summarize this data analysis at the end, we can say that there are definitely major factors that influence the solvage rate of a certain crime. It is
very important to not treat every crime the same, the major categories of crimes differ a lot in their solvage. It is very possible, that certain crime
categories are investigated more intense by the police, as well as some crimes are especially easy or hard to solve. <br/>
It is also not possible to take all numbers for whole Germany, many provinces differ from one another in many aspects, so a detailed analysis for those
is necessary. <br/>
The last point, that this analysis showed, was that suspects play a major roll in solving crimes and that a majority of them are male, but also that
certain crimes have a tendency towards repead offenders.
