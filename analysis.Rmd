---
title: "BKA_Kriminalstatistik_2019_data_analysis"
author: "Peter von Bodelschwingh, Joshua Gawenda"
date: "12 12 2020"
output:
  html_document:
    df_print: paged
  pdf: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(dplyr)
library(sjmisc)
library(tidyverse)
library(readxl)
library(GDINA)
#install.packages("geojsonio")
#install.packages("mapproj")
#install.packages("scales")
library(geojsonio)
library(broom)
#library(scales)
```

# German crime statistics 2019 - A data analysis

## Introduction

In this data analysis we are going to take a look at the German crime statistics from the year 2019, the special version we are looking at also
shows the crime distribution about different sized cities. The data is divided for all 16 German provinces and into a general section for the whole
country. <br/>
There are also multiple files that help to determine what can be deducted from a data analysis of this set and what is out of scope. <br/>
The data set is taken from the  "Bundeskriminalamt" - the leading institution of crime detection in Germany. <br/>
The original data set can be found here: 
[BKA Dataset](https://www.bka.de/DE/AktuelleInformationen/StatistikenLagebilder/PolizeilicheKriminalstatistik/PKS2019/PKSTabellen/LandFalltabellen/landFalltabellen.html) <br/>
This data set only shows absolute numbers for all provinces in Germany, so to make the statistics comparable over all the different provinces we
also included another data set, that shows the number of inhabitants for every province and the whole country. <br/>
This data set can be found here: 
[German Inhabitants](https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.html) <br/>
Later you will see that we imported on more data set. This one is generated out of a PDF from the documentation for the first data set and therefore
is not available in the format we have online. It holds information about the so called sum keys of the main data set and is later used to decouple
the main data set.

### Sum Keys

To understand a big part of the analysis you have to understand what sum keys are, this is what we are going to describe in this chapter. <br/>
A key in the data set we use represents one type of crime, every crime has its own distinct key. Some crimes are just a composite of multiple other
crimes, their key is then called a sum key, so every crime that is not a sum, has no children but only a parent crime. <br/> 
A key is made out of six digits, where every digit is read as a single one from left to right. In general every key that starts with the same digits 
is part of the same sum key, the sum key is then identified with the same starting digits followed by zeros. However this is not true in every case, 
some keys that end with zero have no children and therefore are no sum keys, the sum key for those keys is in most cases represented by a "*"-sign. <br/>
One general section of crimes is therefore represented by only the first digit. There are also two whole sections of keys that are only made out of
sum keys and are used by the police to study different fields of crimes.

## Import

The first step to every data analysis is to import the data into your project. The data we chose is available as an excel file so we use the 
package "readxl" to import the sets from excel. For the import to work properly we had to remove some unnecessary header lines in the main data
set, we did that by specifying a "range" in the read_excel command. <br/>
The "SumKeys" data set was created by us, therefore every optimization for importing where made while creating the set. <br/> 
Finally we also import the data set for the inhabitants, since this data set is not part of the main analysis, we decided to also edit it in
excel to optimize it for the import and remove data that we do not need for the analysis.
 
```{R Import}
originalSet <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Data",range = "A8:U18777") 
sumKeysImport <- read_excel("DataSets/PKS_crime_statistics.xlsx", sheet = "Sumkeys")
germanInhabitants <- read_excel("DataSets/02-bundeslaender.xlsx", sheet = "inhabitants")
```

### Renaming

After the import, the names of the original data set got lost and are not descriptive anymore. To assure that the usage later on is more understandable
we renamed the data set.

```{r Renaming}
originalSet <- originalSet %>% 
                rename(
                  key = '1',
                  crime = '2',
                  province = '3',
                  detected_cases = '4',
                  detected_cases_percent = '5',
                  not_executed_cases = '6',
                  not_executed_cases_percent = '7',
                  distribution_under_20K = '8',
                  distribution_20K_to_100K = '9',
                  distribution_100K_to_500K = '10',
                  distribution_over_500K = '11',
                  distribution_unknown = '12',
                  gun_threatened = '13',
                  gun_shot = '14',
                  solved_cases = '15',
                  solved_cases_percent = '16',
                  suspects = '17',
                  male = '18',
                  female = '19',
                  non_german ='20',
                  non_german_percent = '21'
                  )
```
 
As a last step of importing, we can merge the inhabitants data frame into the original data frame for the analysis. This would allow to use the
number of inhabitants easily in any calculation from here on.

```{r Merge_Inhabitants}
originalSet <- merge(originalSet, select(germanInhabitants, "province", "inhabitants"), by = "province") %>% arrange(key)
head(originalSet)
```

## Tidy

This next step includes checking for missing values and in our case splitting the data frames up, so that the provinces are decoupled from the values
for whole Germany. We also need to remove all sum keys from the actual capture keys.

### Check non NA

If the data set contains empty values they are marked as NA and have to be handled somehow. However, this data set has no such values as shown in the
next code chunk. Here we filter for NA values in any column and check if the row count is 0 in the end.

```{r NA_Check}
nrow(filter_at(originalSet, 1:21, any_vars(is.na(.)))) == 0
```

### Decouple

Like describe in the "Sum Key" section, the original data set is a mix of sum keys and capture keys. In order to not count twice in some operations
we remove the sum keys from the capture keys, so the main analysis can focus on the capture keys. <br/>
The sum keys set that we imported has two columns, one for the key and one with the value "Y" for every capture key and a "N" for every sum key. The 
two key sections that are only made out of sum keys are not reported in the set, so can be handled as "NA".

```{r Filtering_Keys}
captureKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key))
sumKeys <- sumKeysImport %>% filter(!grepl("Y",is_sum_key))
```

To check, that the keys after the filter are all correct, we created a small algorithm that checks for sum keys based on the the rules described in
the "Sum Keys" chapter. The code takes a while to run, that is why we saved the result in a vector. 

```{r Manual_Sum_Key_Check}
#filteredKeys <- c()
#for (key1 in captureKeys$key) {
#  if (!key1 %in% filteredKeys) {
#    for (key2 in captureKeys$key) {
#      if (!key2 %in% filteredKeys) {
#        if (key1 != key2) {
#          if (regexpr(paste("^", str_remove(key2, "0+$"), sep = ""), key1)[1] == 1) {
#            filteredKeys <- append(filteredKeys, key2)
#          }
#        }
#      }
#    }
#  }
#}
filteredKeys <- c("133000","141100","231200","305000","310000","315000","325000","326000","335000","340000","345000","350000","390000","435000","436000","620010","655010","670010","670020","670030")
```

After checking the resulting keys manually we found, that the keys "141100", "231200" and "133000" were marked as capture keys, but are sum keys,
so these keys have to be removed from the capture keys and added to the sum keys.

```{r Correct_Key_Filter}
captureKeys <- sumKeysImport %>% filter(grepl("Y",is_sum_key) & !(key %in% c("141100","231200","133000")))
sumKeys <- sumKeysImport %>% filter(!grepl("Y",is_sum_key) | (key %in% c("141100","231200","133000")))
```

### Filter final data frames

First to reduce duplicate data, we filtered out all crime descriptions into its own data frame, to it can be looked up afterwards.

```{r Category}
crimeCategories <-
  originalSet %>% filter(grepl("[0-9]0{5}", key)) %>%
  select(key, crime) %>%
  unique() %>%
  mutate(key = str_replace_all(key, "00000", ""))

originalSet <- originalSet %>% 
  mutate(category = substring(key, 1,1)) %>% 
  select(category, !category) %>% 
  mutate(category = ifelse(category%in%crimeCategories$key,crimeCategories[as.integer(category)+1,2],"Other"))
```


```{r Crime_Frame}
crimeOverview <- originalSet %>% select(key, crime) %>% unique()
crimeOverview <- arrange(crimeOverview, key)
head(crimeOverview)

originalSet <- originalSet %>% select(!crime)
```
    
## Transform

In the final step before visualizing the data we need to check all the types of the values in each column an change them to a type that is usable
for our analysis. We can also remove and create new Columns if necessary. In the end we are also splitting the original data frame up based on the
provinces and the sum keys determined before.

### Transforming data types

In this step we first check for the different data types we have in every column. We expect to have characters as the key, a factor for the provinces
and every other column should be a number.

```{r Type_Check}
sapply(originalSet, class)
```
Except for the province all values have the type expected, the next step is to transform the province into a factor.

```{r Factorize_Province}
originalSet$province <- as.factor(originalSet$province) 
originalSet$category <- as.factor(originalSet$category)

sapply(originalSet, class)
```

```{r}
c1 <- levels(originalSet$category)[1]
c2 <- levels(originalSet$category)[2]
c3 <- levels(originalSet$category)[3]
c4 <- levels(originalSet$category)[4]
c5 <- levels(originalSet$category)[5]
c6 <- levels(originalSet$category)[6]
c7 <- levels(originalSet$category)[7]
c8 <- levels(originalSet$category)[8]
c9 <- levels(originalSet$category)[9]
```


### Remove calculated columns

Some columns in the data set are just calculated from the given data, so to remove the possibility of a calculation error, we remove these columns
and add them on our own, once they are needed in the analysis.

```{r Select_Useful_Columns}
originalSet <- originalSet %>% select(key, category, province, inhabitants, detected_cases, solved_cases, not_executed_cases, suspects, male, female, non_german, gun_threatened, gun_shot, distribution_under_20K, distribution_20K_to_100K, distribution_100K_to_500K, distribution_over_500K, distribution_unknown)
```
### Split up data

This could also be part of the tidy step, but to reduce the amount of duplicate code for all checks, we split the data at the end. Therefore, we
merge both key frames together with the original data set to separate the sum keys and the capture keys into separate data frames.

```{r Province_Id include=FALSE}
province_ids <- data.frame (province = c("Baden-Württemberg","Bayern","Berlin","Brandenburg","Bremen","Hamburg","Hessen","Mecklenburg-Vorpommern","Niedersachsen","Nordrhein-Westfalen","Rheinland-Pfalz","Saarland","Sachsen-Anhalt","Sachsen","Schleswig-Holstein","Thüringen", "Bundesrepublik Deutschland"),
                  province_id = c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16", "17"))
originalSet <- originalSet %>% merge(province_ids, by = "province")
```

```{r NA_Check}
originalSet %>% filter_at(vars(1:19), any_vars(is.na(.)))
```


```{r Split_Original_In_Sum_Capture}
captureValues <- merge(originalSet, captureKeys) %>% select(!is_sum_key)
sumValues <- merge(originalSet, sumKeys) %>% select(!is_sum_key)
```

With the sum keys now separated, we also divide both data frames into the values for the provinces and the values for whole Germany.

```{r Separate_Province}
germanCaptureSet <-
  captureValues %>% filter(grepl("Bundesrepublik Deutschland", province))
provinceCaptureSet <-
  captureValues %>% filter(!grepl("Bundesrepublik Deutschland", province))

germanSumSet <-
  sumValues %>% filter(grepl("Bundesrepublik Deutschland", province))
provinceSumSet <-
  sumValues %>% filter(!grepl("Bundesrepublik Deutschland", province))
```

To check that the sum keys from before where separated correct we check that the sum of all detected cases in each German set is equal to the sum 
for the detected cases in the province set.

```{r Sum_Key_Check}
sum(germanCaptureSet$detected_cases) == sum(provinceCaptureSet$detected_cases)
sum(germanSumSet$detected_cases) == sum(provinceSumSet$detected_cases)
```

## Prepare Map

Before our analysis can start, we want to display some data on a map, with all German provinces. In this step we prepare the empty map, so that it can
easily be used later on.

```{r}
province_json <- geojson_read("provinces.json", what = "sp")
empty_map <- tidy(province_json, area = "code") %>% mutate(province_id = id)

ggplot(empty_map, aes(long, lat, group = group)) +
  geom_polygon(fill="white", color="grey") +
  theme_void() +
  coord_map()
```



```{r, echo=FALSE}
options(scipen=999)
```
    
## Analyse 

Once the data is imported, tidied and transformed the analysis can start. To first get a general impression over the data that we have, we
used the standard plot command, that creates a simple chart for every column. The data frame that we use for all of the next instructions
is the one with only the values for whole Germany. We first want to find correlations in there. <br/>
To make the first general data analysis more readable, we need to remove the outliers in the data. All the data in this data set is bound to the 
number of detected cases, so if the number of detected cases is 0 all other columns will also be 0 and the other way around, if there is a high
number of detected cases, the possibility of high numbers in the other columns is pretty high (Those cases will be analyzed later on). <br/>
To find the number of detected cases where there are the most cases, we find the mean of detected cases. To do that we use the summary
function of R.

```{r Find_Cases_Mean}
summary(germanCaptureSet$detected_cases)
```

Now we just filter out all rows where the number of detected cases is larger than 7172. For now we are also not interested in the rows where there
are no cases, so we also filter out those.

```{r Filter_For_Number}
cutSet <- filter(germanCaptureSet, detected_cases > 0 & detected_cases <= 7172)
summary(cutSet$detected_cases)
```

To now get a general view on how the data is structured across the data frame we use the plot command, that creates simple plots for every
column in the frame.

```{r}
plot(cutSet, main = "General Overview", col = "#000000", pch = 19)
```

Based on these plots we found a few interesting correlations of data that we now are going to discuss in detail.


############################
############################
Steps:
1. Detected vs Solved
2. Detected/Solved vs Suspects
3. Suspects vs Male/Female/Non_German
4. Distribution
5. Funny Rankings for the end (inhabitants from provinces) -> filter out the 6 top categories (first letter in key)
############################
############################

### Detected Cases vs Solved Cases

The first thing that we noticed was, that there seems to be a relation between the number of detected cases and the number of solved cases. 
Next we enlarge this plot and filter out all rows where the number of solved cases is larger than the number of detected cases, because those would 
be part of a measurement error. We again also remove those cases where the number of detected cases is 0.

```{r Filter_Impossible_Solvage_Rate}
filtered <- filter(germanCaptureSet, detected_cases >= solved_cases & detected_cases > 0)
```

```{r Solvage_Rate}
ggplot(filtered, aes(detected_cases, solved_cases, color = category)) +
  geom_point(show.legend = T) +
  geom_smooth(method = "lm", fullrange = T, show.legend = F) +
  geom_abline(coef = c(0, 1)) +
  labs(title = "Detected vs. Solved", y = "Solved Cases", x ="Detected Cases")
```

This plot shows the relation between detected and solved cases for each major crime category.
The black line displayed in this plot shows the optimal line, where the number of detected cases is equal to the number of solved cases, so those crimes
where every case was solved. The closer a dot or a colored line is to this line, the higher is the solvage rate for the specific crime. <br/>
Lets now take a closer look at the different solvage rates.

```{r Calc_Solvage_Rate}
filtered <- filtered %>% mutate(solvage_rate = solved_cases/detected_cases*100)
ggplot(filtered, aes(category, solvage_rate, fill = category)) +
  geom_boxplot() +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = "Solvage Rate", x = "Rate in Percent")
```

Lets also get the number to these graphs.

```{r}
descr(filter(filtered, category == c2)$solvage_rate)
c2
descr(filter(filtered, category == c3)$solvage_rate)
c3
descr(filter(filtered, category == c4)$solvage_rate)
c4
descr(filter(filtered, category == c5)$solvage_rate)
c5
descr(filter(filtered, category == c6)$solvage_rate)
c6
descr(filter(filtered, category == c7)$solvage_rate)
c7
descr(filter(filtered, category == c8)$solvage_rate)
c8
descr(filter(filtered, category == c9)$solvage_rate)
c9
descr(filter(filtered, category == c4 | category == c5)$solvage_rate)
"Crimes associated with theft"
descr(filtered$solvage_rate)
"General"
```

As can be seen in these plots, there is a high variation in the solvage rate of cases based on their category. In general the solvage rate is very high,
the general median lies at 72% solvage. There are just two exceptions, which are those crimes associated with theft, these lie around 36% solvage rate.
There is also one more crime category that stands out, every crime associated with murder, this has an extremely low inner quartile range which means, 
that the solvage rate there is very consistent. <br/>
When looking back at the plot, there are together with the murder category two more categories, that have a high mean solvage rate and a low inner
quartile range. These categories are special cases in the data set and you could assume that there are specific reasons behind the relatively consistent 
and high solvage rate. <br/> 
<br/>
Now that we analyzed the solvage rate for the different crime categories for whole Germany, we can take a quick look at the rates for the different 
provinces. To keep this part short, we just used the overall mean solvage rate for every province instead of separating it into to different categories.

```{r Solvage Rate}
province_filtered <- provinceCaptureSet %>%
  filter(detected_cases >= solved_cases & detected_cases > 0) %>%
  mutate(solvage_rate = solved_cases / detected_cases * 100) %>%
  select(province_id, solvage_rate) %>% 
  mutate(province_id = as.integer(province_id))

province_filtered <- aggregate(province_filtered, list(province_filtered$province_id),FUN=mean) %>% 
  select(!province_id) %>% 
  rename(province_id = "Group.1") %>% 
  mutate(province_id = as.character(province_id))

map_data = empty_map %>% left_join(., province_filtered, by = "province_id")

ggplot(map_data, aes(long, lat, group = group, fill = solvage_rate)) +
  geom_polygon(color = "black") +
  geom_polygon(data = filter(map_data, province_id == 3), color = "black") +
  geom_polygon(data = filter(map_data, province_id == 5), color = "black") +
  theme_void() +
  coord_map() + 
  labs(title = "Province Mean Solvage Rate", fill = "Rate in percent")
```

When looking on this map, it is clearly visible, that there are five relatively dark provinces on the map, which means, that they have the lowest solvage
rate in Germany. The four out of these provinces are the smallest provinces in Germany while the fifth has no connection in size to the others. <br/>
To now get a better picture on which provinces are the most dangerous, based on their mean solvage rate and their crimes per inhabitant, we create a
second map that shows the crimes per 10.000 inhabitants.

```{r}
province_filtered <- provinceCaptureSet %>%
  filter(detected_cases >= solved_cases & detected_cases > 0) %>%
  mutate(crime_rate = detected_cases / inhabitants * 10000) %>%
  select(province_id, crime_rate) %>% 
  mutate(province_id = as.integer(province_id))

province_filtered <- aggregate(province_filtered, list(province_filtered$province_id),FUN=sum) %>% 
  select(!province_id) %>% 
  rename(province_id = "Group.1") %>% 
  mutate(province_id = as.character(province_id))

map_data = empty_map %>% left_join(., province_filtered, by = "province_id")

ggplot(map_data, aes(long, lat, group = group, fill = crime_rate)) +
  geom_polygon(color = "black") +
  geom_polygon(data = filter(map_data, province_id == 3), color = "black") +
  geom_polygon(data = filter(map_data, province_id == 5), color = "black") +
  theme_void() +
  coord_map() + 
  labs(title = "Province Accumulated Crimes", fill = "Crimes per 10.000 inhabitants")
```

And again, the three lightest spots on the map, therefore the ones with the highest crime rate, are the three smallest provinces in Germany. Also the 
other two provinces from the last map have a relatively light tint, a bit above the average, which could indicate a possible relationship between
the crime rate and the solvage rate.

### Solved Cases vs Suspects

Now that we analyzed the aspects of the solvage rate in Germany, We wanted to have a look at the suspects involved in solving a case. How many suspects
do you need to close a case? Are more suspects better for solving the case? <br/>
One note upfront for this, if a group of people is committing a crime together, every one of them will get their own law suite, which means that every 
crime listed in our data set has only one person that executed it. Also a person will only be listed once per crime, so if person x committed fraud twice
and theft once, then he will appear both in the fraud and theft section only once as a suspect. This means, that a crime with 100% solvage rate and
a lower amount of suspects than detected cases is only explainable by repead offenders. <br/>
For sum keys about the suspects the same rules apply, so a suspect that commits multiple crimes for the same sum key will only be counted once, so 
summing up all capture keys contained in a sum key will often lead to a higher number of suspects than what is written in the sum key.

Since we calculated the solvage rate in the last chapter for a filtered dataframe, we are now adding the solvage rate to the germanCaptureSet, to have these values stored, including those values with a solvage rate greater than 100 % to avoid data loss. 
```{r}
germanCaptureSet <- germanCaptureSet %>% mutate(solvage_rate = solved_cases/detected_cases)
```


Note: Points underneath the line are suspected many times
Note: One case -> one person that did it (every one in the case gets its own law suite)

Referring back to our overview again, we also noticed that there seems to be a correlation between solved cases and suspects:
```{r}
ggplot(data = germanCaptureSet, mapping = aes(x = solved_cases,y = suspects)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(x = "Solved cases", y = "Suspects")
```



Since we want to find to calculate the suspects rate later on we have to remove all rows where no crimes have been detected and rows where no suspects could be found because a division would lead to NA values. 
```{r}
solvedCasesFiltered <- filter(germanCaptureSet, detected_cases >0, suspects >0)
```

Lets take a detailed look into the filtered plot now, by differenciating between the categories, because we would like to know why there are a few values deviating: 
```{r}
ggplot(solvedCasesFiltered, aes(solved_cases, suspects, color = category)) +
  geom_point(show.legend = T) +
  geom_smooth(method = "lm", fullrange = T, show.legend = F) +
  labs(title = "Suspects vs. Solved", y = "Solved Cases", x ="Suspects")
```

As can be seen in the plot above, only the blue line which determines crimes against life really deviates in its curve from the others. Crimes against life has a higher suspects rate. Lets calculate the suspects rate per category with the mean: 
```{r}
# mean solved cases by category
sum_solved<- aggregate(x = germanCaptureSet$solved_cases,                # Specify data column
          by = list(germanCaptureSet$category),              # Specify group indicator
          FUN = mean) %>% rename("sum_solved" = x)
# mean amount of suspects by category
sum_suspects<- aggregate(x = germanCaptureSet$suspects,                # Specify data column
          by = list(germanCaptureSet$category),              # Specify group indicator
          FUN = mean) %>% rename("sum_suspects" = x)
# merge and divide 
(percentage_solved <- merge(sum_solved,sum_suspects, by = "Group.1") %>% rename("category" = "Group.1")  %>% mutate("suspects per case" = sum_suspects/sum_solved))

```
Now it is very interesting to see that the category with the highest suspects per case rate is also the one with the highest solvage rate. Because these calculations are only grouped by the category, we are looking into the whole dataframe now by using a boxplot: 

```{r}
solvedCasesFiltered <- solvedCasesFiltered %>% mutate(suspects_rate = suspects/solved_cases)
ggplot(solvedCasesFiltered, aes(category, suspects_rate, fill = category)) +
  geom_boxplot() +
  labs(title = "Suspects Rate", x = "Rate by category", y = "Suspects rate")
```
As already been explained in the chapter introduction there can be repead offenders. This is the case when we have a solvage rate of 100% for a crime and a suspects rate below 1,0. The points of the boxplot that are below 1 are represented by this table: 
```{r}
temp <- solvedCasesFiltered %>% filter( solvage_rate ==1, suspects_rate < 1)
(merge(temp,crimeOverview, by = "key"))
```
#### Outliers analysis
In all of these rows we have a certain amount of repead offenders. Unforunately there is no pattern, thats why we will check the outliers above the IQR now. Lets do this by looking at the "Sonstige Straftatbestände" category. 
```{r}
temp <- solvedCasesFiltered %>% filter(category == c3, suspects_rate >= 2)
temp <- merge(temp,crimeOverview, by = "key")
(temp <- temp %>% select(crime,suspects_rate ) %>% arrange(desc(suspects_rate)))
```
Researching the upper four crimes showed that these crimes can only be commited by more than one suspects. This explains the high outliers. 
Lets see whether this applies to the upper outliers of the "Strafrechtliche Nebengesetze" category as well. Unfortunately its not as easy as for the last category to determine the upper outliers, thats why we are calculating this now, by filtering out all values above Inter quartile 3*1,5, that determines the start of the upper outliers: 
```{r}
# filter category == "Strafrechtliche Nebengesetze"
upper_outlier_start <- solvedCasesFiltered %>% filter(category == c6)
# determine 3rd quartile ending at 75%
upper_outlier_start <-quantile(upper_outlier_start$suspects_rate, .75)
# retrieve IQR using IQR()
IQR <- IQR(solvedCasesFiltered$suspects_rate[solvedCasesFiltered$category == c6])
# multiply IQR*1,5 and add to 3rd quantile to get outliers start
(upper_outlier_start <- upper_outlier_start+1.5*IQR)
```


```{r}
temp <- solvedCasesFiltered %>% filter(category == c6, suspects_rate > upper_outlier_start)
temp <- merge(temp,crimeOverview, by = "key")
(temp <- temp %>% select(crime,suspects_rate,detected_cases, solved_cases ) %>% arrange(desc(suspects_rate)))
```
Looking at the two highest outliers, we could assume that the suspects are the same suspects for both crimes. If we dive deeper into the dataframe we can look at male, female and non german: 
```{r}
(temp <- solvedCasesFiltered %>% filter(suspects_rate == 8) %>% select(suspects_rate,detected_cases, solved_cases, male, female, non_german ) %>% arrange(desc(suspects_rate)))

```
These details confirm that the suspects are not the same suspects for the crimes. Therefore the outlier is also a value we should keep in our frame when we want to move on with the analysis. 
#### Linear model:
Since we checked the highest outliers now we can try to generate a linear model, in order to check how the amount of suspects influences the amount of solved cases, by first refilter the germanCaptureSet, to rows with cases only:
```{r}
germanCasesSet <- germanCaptureSet %>% filter(detected_cases> 0)
```
The reason we are not taking the solvedCasesFiltered set is because we removed values without suspects. Modelling this data would lead to deviating, not accurate results. Now, we first check the correlation of suspects and solved cases in the germanCasesSet: 
```{r}
cor(germanCasesSet$solved_cases, germanCasesSet$suspects)
```
The correlation leads to the question: Is the amount of solved cases affected by the amount of suspects? 
First step: Generate linear model using lm():
```{r}
reg <- lm(germanCasesSet$solved_cases~germanCasesSet$suspects)
summary(reg)
```
First of all, the min and high values are deviating far from the first and third quartile. This signals us that there are outliers already. But lets move on. What we can see is, When solved cases are zero, the amount of suspects is -170,71 and with each increase of solved cases by one, the amount of suspects rises by 1,13. Since we can see already that the standard error of solved cases is about 127 cases, the deviation is way too high, at least for the intercept of our model. This impression is confirmed by the probability value, that determines whether suspects do not influence the amount of solved cases, also known as null hypthesis: For solved cases at the intercept, it is 0,18 and therefore, higher than the cut off score of 0,05. The evidence is way too low to asssume that suspects really affect the amount of cases, but only at this point.This is confirmed by the second probability value: This one is way below 0,05, the null hypothesis is not confirmed.  Probably the variation of suspects and solved cases is just too high, the degree of freedom shows a high population.  
Also, the residual standard error follows the impression of the SSE: It is very high, although we have a high amount of freedom degrees. (710 rows - 2 variables in the model = 708 DF). Looking at the R squared tells us here that, although the RSE and the SSE seem to be very high, the model determines a good fit: 0,9776. Therefore, 97 percent of the variation is explained by the model. What we have learned from the outliers analysis as well was that there can be crimes that can only be commmited my more than one suspects. This also explains the deviation a bit. In conclusion, the model tells us a good fit, but its needs to be taken into account that the amount of DF is very high, therefore the variation can be and is influenced by other variables. 

In the next step, we will look at the residuals to see the spread:
Proof of residuals: 
Residuals are the difference of the observed y values (solved cases) predicted values of y  from our dataframe. If the prediction is higher than the observed value, the residual is negative. If predicted and observed value are equal the residual is zero. 
Therefore if we sum up the collected residuals we know whether our formula is accurate or not: 
```{r}
sum(reg$residuals)
```
The result is almost 0, which is a good sign for the spread of residuals.
Still the frequency of residuals up and below zero has to be analyzed: 
```{r}
hist(reg$residuals)
```
It confirms the sum and our impression that the suspects are not a mainfactor for the change of solved cases. We still take a look into the plot of the reg: 
```{r}
par(mfrow=c(2,2))
plot(reg)
par(mfrow=c(1,1))
```
#### Residuals vs Fitted 
As we can see, the red line seems to not follow the dotted line because of the non equal spread of positive and negative residuals. Also there are a few observations that highly influence the model, in the following rows of the germanCasesSet: 283,289 and 172. 

#### Normal Q Q 
We can see skewed residuals in the upper and lower quantiles. This tells us what we already thought. The spread inside the population is not equal, especially in the start and the end. This is confirmed when you take a look back at the intercept values of the reg summary. It also explains the high SSE at the intercept.Nevertheless, the curve looks good, it follows the dotted line.  

#### Residuals vs Leverage 
There are a few influential cases here, but only two of them areabove the dotted line: 289 and 172. Since there are a lot of values at the start of the dotted line, we have to be careful interpreting predicted values because: We have four values beneath the red lines which have a high influence. 

#### Scale location
This plot also shows the non equal spread of residuals. As we can see, the red line goes up, this shows that thea ssumption of homoscedasticity is not given. The residuals seem also not to be randomly scattered.

#### Conclusion Solved Cases vs Suspects
Since we have proven a high accuracy of the model with the R squared, we could take the model, but when it comes to the intercept the model is not accurate. This makes sense because we are of course not interested in negative solved cases. Also, the results must be treated with caution, since there can be crimes which require multiple suspects by law and the unequal spread of residuals. However we can try to check the accurary to finish this chapter:Lets get the max value of lets say the third quartile: 
```{r}
q3 <- quantile(germanCasesSet$solved_cases,.75)
val <- filter(germanCasesSet, solved_cases <= q3) 
(max <- max(val$solved_cases))
```
Therefore, 1267 x 1,137880 =1.441,69396 is the predicted amount of suspects. The actual value is: 
```{r}
(value <- germanCasesSet$suspects[germanCasesSet$solved_cases == max])
```
So how much deviation do we have? 
```{r}
# in percentage
(1 - (max/value))*100
```
The deviation of predicted and actual value is about 24 percent. This is a high deviation and it confirmed what we expected earlier: Solved cases are also influenced by other factors, but the amount of suspects definetely has a high influence. In conclusion: The model predictions for a less amount of solved cases are not accurate, as well as when it comes to solved cases at the end because of outliers. 


Note: Points underneath the line are suspected many times
Note: One case -> one person that did it (every one in the case gets its own law suite)

```{r}
(temp <- germanCaptureSet %>% filter(solved_cases/detected_cases==1, suspects < solved_cases) %>% 
   select(key, category, detected_cases, solved_cases, suspects))
crimeOverview %>% filter(key %in% temp$key)
```

```{r Suspects_Per_Case}
par(mfrow=c(2,1))
filtered <- filter(germanCaptureSet, detected_cases >= solved_cases)
plot(filtered$detected_cases, filtered$suspects, 
     xlim = c(0,8000),
     ylim = c(0,8000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Detected Case",
     xlab = "Detected Cases",
     ylab = "Suspects")
abline(coef = c(0,1))

plot(filtered$solved_cases, filtered$suspects, 
     xlim = c(0,8000),
     ylim = c(0,8000),
     col = "#cc0000",
     pch = 19,
     main = "Suspects per Solved Case",
     xlab = "Solved Cases",
     ylab = "Suspects")
abline(coef = c(0,1))
par(mfrow=c(1,1))
```

### Suspects and how they are structured

Now that we analyzed how suspects affect the outcome of an investigation, lets look into what the value behind the suspects is made of. There are three
variables that describe the suspects more detailed: Male, female and non German suspects. Male and female suspects together add up to the total number 
of suspects, while the non German suspects are just a part of it. <br/>
Lets start with looking at the female and male distribution on crimes.

```{r}
ggplot(germanCaptureSet) +
  geom_point(aes(suspects, male), color = "blue") +
  geom_point(aes(suspects, female), color = "red") +
  geom_abline() + 
  labs(title = "Male and Female Suspect Rate", x = "Number of all Suspects", y = "Male(blue) / Female(red)")
```

The graph above shows the number of blue dots as the amount of male suspects, while the red dots represent the amount of female. The black line 
shows the point where there would be a 100% majority of sex from the suspects. <br/>
The plot shows very clear, that the majority of suspects are male. To further analyze this aspect we introduced the "male coefficient" that show
the percentage of males for the crime, the females would then be whats missing to the 100%.

```{r}
maleCoef <- germanCaptureSet %>% filter(suspects > 0) %>% mutate(male_coef = male/suspects)

hist(maleCoef$male_coef, 
     breaks = 20,
     freq = F,
     col = "#cc0000",
     main = "Male coefficient in all suspects",
     xlab = "Male percentage")

curve(dnorm(x, mean = mean(maleCoef$male_coef), sd = sd(maleCoef$male_coef)),
      col = "grey",
      lwd = 2,
      add = T)

lines(density(maleCoef$male_coef, adjust = 1), col = "blue", lwd = 2)
lines(density(maleCoef$male_coef, adjust = 3), col = "lightblue", lwd = 2)
rug(maleCoef$male_coef, lwd = 1, col = "lightblue")
```

This histogram shows the distribution of the male coefficient in the data frame, it is overlayed with a normal distribution (gray line) and a line
representing the density of the data (blue = raw, light blue = smoothed). All the curves follow the chart pretty close and display a clear cluster
around the mean value.

```{r}
descr(maleCoef$male_coef)
```

```{r}
reg <- lm(maleCoef$male~maleCoef$suspects)

summary(reg)

plot(reg)
```


The average change of a man being suspected for a crime is 83 percent. Because the mean and the median are very close, the value given is an accurate
view of the reality. This data however does not show how many crimes are done my men, since they could also be wrongly accused. <br/>
To find the male coefficient for crimes that where really done by men, we have to look at the values that have the same amount of suspects as the
amount of solved cases, because this means that every case got a single suspect and therefore no wrong accusations.

```{r}
par(mfrow=c(1,2))
maleCoefFiltered <- maleCoef %>% filter(solved_cases==suspects)

boxplot(maleCoef$male_coef,
        col = "#cc0000",
        main = "All suspected",
        ylab = "Male percentage",
        xlab = paste(nrow(maleCoef)," rows"))
boxplot(maleCoefFiltered$male_coef,
        col = "#cc0000",
        main = "Rightful suspected",
        ylab = "Male percentage",
        xlab = paste(nrow(maleCoefFiltered)," rows"))
par(mfrow=c(1,1))
```

Comparing the two different graphs, they have many points in common and are not that different from one another. Both show a high rate of crimes
with more male suspects and only outliers for crimes with a high female suspection rate. The inner quartile range of both plots is almost the
same height, the lower whisker is only a bit smaller than the one on the right side. <br/>
Effectively the right plot displays with w much fewer observations a close picture to the left one. So what is displayed on the left is not far off
by suspecting the correct gender for a crime. <br/>
<br/>
Lastly lets look at the male coefficient distributed over the different provinces of Germany.

```{r}
province_filtered <- provinceCaptureSet %>%
  filter(suspects > 0) %>%
  mutate(male_coef = male / suspects / 699) %>%
  select(province_id, male_coef) %>% 
  mutate(province_id = as.integer(province_id))

province_filtered <- aggregate(province_filtered, list(province_filtered$province_id),FUN=sum) %>% 
  select(!province_id) %>% 
  rename(province_id = "Group.1") %>% 
  mutate(province_id = as.character(province_id))

map_data = empty_map %>% left_join(., province_filtered, by = "province_id")

ggplot(map_data, aes(long, lat, group = group, fill = male_coef)) +
  geom_polygon(color = "black") +
  geom_polygon(data = filter(map_data, province_id == 3), color = "black") +
  geom_polygon(data = filter(map_data, province_id == 5), color = "black") +
  theme_void() +
  coord_map() + 
  labs(title = "Province Accumulated Percentage of Male Accusations", fill = "Male percentage")
```

Now lets also take a look at the percentage of non German suspects and how they are distributed across the provinces. 

```{r}
province_filtered <- provinceCaptureSet %>%
  filter(suspects > 0) %>%
  mutate(male_coef = non_german / suspects / 699) %>%
  select(province_id, male_coef) %>% 
  mutate(province_id = as.integer(province_id))

province_filtered <- aggregate(province_filtered, list(province_filtered$province_id),FUN=sum) %>% 
  select(!province_id) %>% 
  rename(province_id = "Group.1") %>% 
  mutate(province_id = as.character(province_id))

map_data = empty_map %>% left_join(., province_filtered, by = "province_id")

ggplot(map_data, aes(long, lat, group = group, fill = male_coef)) +
  geom_polygon(color = "black") +
  geom_polygon(data = filter(map_data, province_id == 3), color = "black") +
  geom_polygon(data = filter(map_data, province_id == 5), color = "black") +
  theme_void() +
  coord_map() + 
  labs(title = "Province Accumulated Percentage of Non German Accusations", fill = "Non German percentage")
```

Both maps show a lower percentage in the eastern states of Germany and mark the same provinces with a higher percentage of suspects. Comparing the two
maps, it is clear that there is a high change, that a non German suspect is also male.

### Distribution

We now know how the amount of suspects is connected to a solved case and how the suspects are split up into male and female. Now in this last analysis 
chapter we take a look on the distribution of crimes. We try to find a correlation between the distribution of crimes and their solvage. <br/>
The problem in our data set is, that we do not know the amount of solved cases per distribution, we only know the amount of detected cases, for the
further analysis we just look at the relative solvage rate based on the number of detected cases in the distribution. <br/>
To start of with this chapter we have to modify our data frames, so that the columns for the different distributions are getting reduced into two
columns, one for the distribution name as a factor and one for the corresponding value.

```{r Melting}
#install.packages("reshape")
library(reshape)

meltedGerman <- melt(filter(germanCaptureSet, detected_cases > 0), id = c("key", "category", "province", "inhabitants", "detected_cases", "solved_cases", "not_executed_cases", "suspects", "male", "female", "non_german", "gun_threatened", "gun_shot", "province_id", "solvage_rate"))
meltedGerman$value <- as.integer(meltedGerman$value)
meltedGerman <- meltedGerman %>%  mutate(rel_solvage = value/detected_cases * solved_cases/detected_cases, relevance = value/detected_cases) %>% 
  mutate(shift = relevance - rel_solvage)

meltedProvince <- melt(filter(provinceCaptureSet, detected_cases > 0), id = c("key", "category", "province", "inhabitants", "detected_cases", "solved_cases", "not_executed_cases", "suspects", "male", "female", "non_german", "gun_threatened", "gun_shot", "province_id"))
meltedProvince$value <- as.integer(meltedProvince$value)
meltedProvince <- meltedProvince %>%  mutate(rel_solvage = value/detected_cases * solved_cases/detected_cases, relevance = value/detected_cases) %>% 
  mutate(shift = relevance - rel_solvage)

detach("package:reshape", unload=TRUE)
meltedGerman <- meltedGerman %>% rename(distribution = "variable", distribution_value = "value")
meltedProvince <- meltedProvince %>% rename(distribution = "variable", distribution_value = "value")
```

After melting the data sets, we also introduced new calculated variables. We calculated the relevance of a distribution, which represents the percentage 
on all detected cases. The relative solvage rate is the solvage rate of the crime multiplied by the distributions relevance. The maximum value for the 
relative solvage rate is the relevance, so we also calculated the difference of to the optimal value as the shift.

```{r Relevance_Solvage}
#install.packages("gridExtra")
library(gridExtra)
grid.arrange(
  ggplot(filter(meltedGerman, distribution_value > 0)) +
    geom_boxplot(aes(relevance, distribution)) +
    geom_boxplot(aes(rel_solvage, distribution), color = "red", fill = "transparent") +
    labs(
      title = "Distributions relevance and relative solvage",
      y = ""
    ),
  ggplot(filter(meltedGerman, distribution_value > 0)) +
    geom_boxplot(aes(shift, distribution)) +
    labs(
      title = "Distributions variance from optimal solvage",
      y = ""
    ),
  nrow = 2
)
```

On the first glance the first graph looks very busy, but it is important to overlay the different boxplots. The underlying black one displays the
distributions relevance, while the red overlay represents the relative solvage rate. The first thing to notice in these graphs is, that the
distribution unknown is an outlier, it has a small relevance but an extremely high relative solvage rate, they almost overlay perfectly. When looking
at the second plot, this picture solidifies, there we can see the extremely low shift from an optimal solvage rate. <br/>
Going back to the other values, it is visible, that the relevance is more consistent for distributions from 20K to 100K and from 100K to 500K even though
the relevance itself varies much between those two. It seems like the most crimes occur in distributions from under 20K to 100K followed by the 
distribution over 500K. Based on the inner quartile range of the solvage rate it seems, that the consistency for the solvage rate increases for 
distributions with lower relevance. <br/>
Looking again at the second graph it is also visible, that the distribution from 100K to 500K has a lower shift than the other three (except for the 
unknown), all the others are very similar, but this one hits to a higher relative solvage rate. <br/>
Now, there seems to be a relation between the relevance and solvage rate of a distribution, lets look closer at this.

```{r}
ggplot(meltedGerman, aes(relevance, rel_solvage, color = category)) +
  geom_point() +
  geom_smooth(method="lm") +
  facet_wrap(~distribution)
```

Above we plotted the relevance and solvage for each distribution and every category and added a optimal line to every group. Going back the the boxplots
from before, the distribution unknown should have the lowest overall relevance and a high solvage rate. The facet of the current graph does not show
that, there is only one category visible with a high solvage rate, while the others are very low. So possibly all the values with a lower relevance are
clustered at one end where the solvage rate is not visible in this view. <br/>
On the other hand the distribution 100K to 500K exactly shows what was displayed before, many values are clustered with a low relevance, but the
solvage rate in many categories is very high. The only categories with a lower solvage rate are related to theft, like described in the chapters above.
But while the solvage rate of crimes related to theft is low, they always have a relevance of under 50%. <br/>
Whats also interesting to see is, that crimes related to property and counterfeiting have relevance of under 50% and a high solvage rate for most 
distributions, except for the distribution over 500K and also the outlier unknown. <br/>
In general there seems to be positive relation between the relevance and relative solvage rate, the standard error of the optimal line in the plots
are in most cases low, except for some outliers.

```{r}
#install.packages("car")
library(car)
reg <- lm(rel_solvage~relevance+category, data = meltedGerman)
reg2 <- lm(rel_solvage~relevance, data = meltedGerman)

vif(reg)

summary(reg)
summary(reg2)

anova(reg2, reg)

plot(reg)
```


Now lets also look at how the distributions are present in the different provinces.

```{r}
province_filtered <- meltedProvince %>%
  filter(relevance > 0, rel_solvage <= 1)

ggplot(province_filtered, aes(relevance))  +
  geom_line(
    aes(y = ..density..),
    lwd = 1,
    stat = 'density',
    adjust = 3,
    col = "red"
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(province_filtered$relevance),
      sd = sd(province_filtered$relevance)
    ),
    lwd = 1,
    col = "blue"
  ) +
  geom_histogram(aes(y = ..density.., fill = distribution), alpha = 0.4) +
  facet_wrap( ~ province)
```

These plots show in a way the amount of larger and smaller cities of the different provinces. It is visible, that distributions over 500K are not present
in every province and wherever they are they seem to have an impact on the relevance density. Every plot has the same blue line which represents
the normal distribution of the complete data frame, while the red line shows the density curve of all distributions for the specific province. <br/>
All provinces without any distribution over 500K has a red line that closely follows the blue line, while the other provinces are shifted. All other
provinces except for the Berlin, Bremen and Hamburg are shifted to the left meaning, that they have more distributions with a lower relevance. This 
however makes sense since those provinces where there are more types of distributions, have to split all their detected cases with more distributions. <br/>
Some provinces have an equal spread of relevance for the different distributions, but for example Schleswig-Holstein, Bayern, Brandenburg and 
Rhainland-Pfalz have a higher relevance for all their distributions under 20K, even though from the height of the bars it does not seem like they have an 
extremely high amount of these distributions. <br/>
Its also interesting to see, that the distribution unknown category has a mostly equal spread over all the provinces, while Berlin does not have any
unknown distributions. To be clear, Berlin, Hamburg and Bremen are special cases since they are provinces and cities in one, but for some reason
Berlin stands out of even these three.

```{r}
ggplot(province_filtered, aes(rel_solvage))  +
  geom_line(
    aes(y = ..density..),
    lwd = 1,
    stat = 'density',
    adjust = 3,
    col = "red"
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(province_filtered$rel_solvage),
      sd = sd(province_filtered$rel_solvage)
    ),
    lwd = 1,
    col = "blue"
  ) +
  geom_histogram(aes(y = ..density.., fill = distribution), alpha = 0.4) +
  facet_wrap( ~ province)
```

We also created the same plots with the relative solvage rate instead of the relevance, with the same blue and red curve on top. The same observation
where the red line follows the blue for all provinces without distributions of over 500K can be made here. <br/>
This time the plots look more ordered after color and height, meaning that every plot starts with a high density of low solvage rates that slowly decreases
while the solvage rate increases. In general the density for the unknown distributions decreases, then it seems like the higher distributions start
decreasing their density first. At the end of every plot there is a small bump of cases with a 100% relative solvage rate. <br/>
Except for the before named three city-provinces, there are two exceptions to the observations from above, first in Sachsen the solvage rate for
the distributions over 500K has a sudden bump around the mean of the normal distribution and second in Nordrhein-Westfalen the distributions under
20K fall of very quickly, together with the distribution unknown.

```{r Distribution_Crimes}
ggplot(meltedGerman, aes(detected_cases, distribution_value, color = category)) +
  geom_point() +
  geom_smooth(method="lm", fullrange = T) +
  facet_wrap(~distribution)
```

