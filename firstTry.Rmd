---
title: "BKA_Kriminalstatistik_2019_data_analysis"
author: "Peter von Bodelschwingh, Joshua Gawenda"
date: "12 12 2020"
output:
  pdf_document: default
  pdf: default
---

```{r setup, include=FALSE}
library(readxl)
library(dplyr)
library(GDINA)
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction
This dataset is taken from the  Bundeskriminalamt - the leading institution of crime detection in germany.
The original dataset can be found here: 
[BKA Dataset](https://www.bka.de/DE/AktuelleInformationen/StatistikenLagebilder/PolizeilicheKriminalstatistik/PKS2019/PKSTabellen/LandFalltabellen/landFalltabellen.html?nn=130872)

It contains data about all crimes recorded in 2019. This analysis divides the statistics into all 16 german provinces. The goal of this analysis is to present the differences between these provinces by categories of crime, population, sucessful and non successfull enlisted crimes. Further documentation is provided in the "Interpretationshilfen" subfolder of this project.  

The original excel file can be found in this repository as well. See "OriginalDataSet". 


## Import
The first step we did was to divide the Overview Sheet into subsheets that contain all data by the province. In order to make sure the importing into R is done correctly, we removed line one til seven so only the Nr of the column is recognized as header. 
 
```{r}
library(readxl)
originalSet <- read_excel("DataSets/LA-F-01-T01-Laender-Faelle_xls.xlsx", 
range = "A8:U18777") 

```
### Decouple
 

Germany datasheet
```{r}
gerSet <- originalSet %>%  filter(grepl('3','Bundesrepublik Deutschland'))
gerSet
```


## Tidy
### Check non null
First of all we proof that the original Set does not contain any null values: 
```{r}
nonNullValue <- originalSet %>% filter_at(vars(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21),any_vars(is.na(.)))
nrow(nonNullValue)
```
### Remove calculated columns
Since we want to keep our analysis as performant as possible we also remove all calculated columns:
```{r}
originalSet <- originalSet %>% select('1':'4','15','6','8':'14','17':'20')
```

## Rename columns
as you can see here the column names are not that descriptive anymore: 
```{r}
names(originalSet)
```
Thats why we are going to rename the columns back to the original name of the dataset now. 
```{r}
originalSet <- originalSet %>% 
                rename(
                  key = '1',
                  crime = '2',
                  province = '3',
                  detected_cases = '4',
                  not_executed_cases = '6',
                  distribution_under_20K = '8',
                  distribution_20K_to_100K = '9',
                  distribution_100K_to_500K = '10',
                  distribution_over_500K = '11',
                  distribution_unknown = '12',
                  gun_threatened = '13',
                  gun_shot = '14',
                  solved_cases = '15',
                  suspects = '17',
                  male = '18',
                  female = '19',
                  non_german ='20'
                  )
```




## Creating subsets of the provinces
After proving the set does not contain null values, we will remove all entrys that show total sums for germany and not for a province. See "




Second: we proof that the sum of all datasets is equal to origin: 

Third: we remove columns that can be calculated / TODO: do we have them? 

## Transform
## Visualise 
## Model 




